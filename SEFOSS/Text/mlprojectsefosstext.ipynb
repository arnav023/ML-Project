{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11224280,"sourceType":"datasetVersion","datasetId":7010132}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport numpy as np\nimport pandas as pd\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    confusion_matrix,\n)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom transformers import BertTokenizer, BertModel\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport pickle\nimport shutil\nimport time\nfrom collections import Counter\n\n# Set random seed for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\nset_seed(42)\n\n# Create directory for saving results\nSAVE_DIR = \"SEFOSS_TEXTData\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Data loading and preprocessing\nclass YahooAnswersDataset(Dataset):\n    def __init__(self, root_dir, max_files=1000, tokenizer=None, max_length=128):\n        self.root_dir = root_dir\n        self.max_files = max_files\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.data = []\n        self.labels = []\n        self.files_per_class = {}\n\n        # Load data from each class folder\n        for class_id in range(10):  # 10 classes in Yahoo Answers\n            class_folder = os.path.join(root_dir, str(class_id))\n            if os.path.exists(class_folder):\n                files = os.listdir(class_folder)[:max_files]\n                self.files_per_class[class_id] = len(files)\n\n                for file_name in files:\n                    file_path = os.path.join(class_folder, file_name)\n                    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                        text = f.read()\n                        self.data.append(text)\n                        self.labels.append(class_id)\n\n        # Create a DataFrame for easier manipulation\n        self.df = pd.DataFrame({\"text\": self.data, \"label\": self.labels})\n\n        # Print stats\n        print(f\"Loaded {len(self.data)} files from {root_dir}\")\n        print(f\"Files per class: {self.files_per_class}\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data[idx]\n        label = self.labels[idx]\n\n        if self.tokenizer:\n            encoding = self.tokenizer(\n                text,\n                truncation=True,\n                padding=\"max_length\",\n                max_length=self.max_length,\n                return_tensors=\"pt\",\n            )\n            return {\n                \"input_ids\": encoding[\"input_ids\"].squeeze(),\n                \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n                \"label\": torch.tensor(label, dtype=torch.long),\n            }\n        else:\n            return text, label\n\n    def get_text(self, idx):\n        return self.data[idx]\n\n    def get_balanced_subset(self, n_per_class):\n        \"\"\"Get a balanced subset with n_per_class samples from each class\"\"\"\n        subset_data = []\n        subset_labels = []\n\n        for label in range(10):\n            indices = [i for i, l in enumerate(self.labels) if l == label]\n            if len(indices) > n_per_class:\n                selected_indices = random.sample(indices, n_per_class)\n            else:\n                selected_indices = indices\n\n            for idx in selected_indices:\n                subset_data.append(self.data[idx])\n                subset_labels.append(self.labels[idx])\n\n        subset_df = pd.DataFrame({\"text\": subset_data, \"label\": subset_labels})\n\n        # Create a new dataset with the subset\n        subset_dataset = YahooAnswersDataset(\n            self.root_dir,\n            max_files=0,\n            tokenizer=self.tokenizer,\n            max_length=self.max_length,\n        )\n        subset_dataset.data = subset_data\n        subset_dataset.labels = subset_labels\n        subset_dataset.df = subset_df\n\n        return subset_dataset\n\n\ndef clean_text(text):\n    \"\"\"Basic text cleaning\"\"\"\n    # Convert to lowercase\n    text = text.lower()\n    # Remove special characters and digits\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n    # Remove extra whitespace\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n\n# Model definition\nclass TextClassifier(nn.Module):\n    def __init__(self, bert_model_name=\"bert-base-uncased\", num_classes=10):\n        super(TextClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(bert_model_name)\n        self.dropout = nn.Dropout(0.1)\n        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n        x = self.dropout(pooled_output)\n        logits = self.fc(x)\n        return logits\n\n    def get_features(self, input_ids, attention_mask):\n        \"\"\"Extract features from the model for feature consistency\"\"\"\n        with torch.no_grad():\n            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n            pooled_output = outputs.pooler_output\n        return pooled_output\n\n\n# SeFOSS implementation based on the algorithms in the image\nclass SeFOSS:\n    def __init__(\n        self, num_classes=10, device=device, backbone_model_name=\"bert-base-uncased\"\n    ):\n        # Models\n        self.backbone = TextClassifier(\n            bert_model_name=backbone_model_name, num_classes=num_classes\n        ).to(device)\n\n        # Tokenizer\n        self.tokenizer = BertTokenizer.from_pretrained(backbone_model_name)\n\n        # Parameters\n        self.num_classes = num_classes\n        self.device = device\n\n        # Hyperparameters\n        self.lr = 2e-5\n        self.batch_size = 16\n        self.lambda_s = 1.0  # strong augmentation weight\n        self.lambda_w = 1.0  # weak augmentation weight\n        self.lambda_f = 1.0  # feature consistency weight\n        self.lambda_e = 1.0  # entropy weight\n        self.wp = 1.0  # pretraining weight\n        self.ws = 1.0  # strong augmentation weight\n        self.ww = 1.0  # weak augmentation weight\n        self.wu = 1.0  # unlabeled weight\n\n        # Thresholds\n        self.tau_d = 0.95  # confidence threshold\n        self.tau_s = 0.95  # strong augmentation threshold\n        self.tau_w = 0.8  # weak augmentation threshold\n\n        # Optimizer\n        self.optimizer = optim.AdamW(self.backbone.parameters(), lr=self.lr)\n\n        # Create directory for saving\n        self.save_dir = SAVE_DIR\n\n    def text_augmentation(self, text, augmentation_type=\"weak\"):\n        \"\"\"\n        Text augmentation for SeFOSS\n        weak: random word deletion\n        strong: random word swap + deletion\n        \"\"\"\n        if augmentation_type not in [\"weak\", \"strong\"]:\n            return text\n\n        words = text.split()\n        if len(words) <= 3:  # Don't augment very short texts\n            return text\n\n        if augmentation_type == \"weak\":\n            # Randomly delete words (15% chance)\n            new_words = [word for word in words if random.random() > 0.15]\n            if not new_words:  # Ensure we don't delete all words\n                new_words = [random.choice(words)]\n\n        elif augmentation_type == \"strong\":\n            # Randomly delete words (20% chance)\n            new_words = [word for word in words if random.random() > 0.2]\n            if not new_words:  # Ensure we don't delete all words\n                new_words = [random.choice(words)]\n\n            # Randomly swap words (15% of pairs)\n            for i in range(len(new_words) - 1):\n                if random.random() < 0.15:\n                    new_words[i], new_words[i + 1] = new_words[i + 1], new_words[i]\n\n        return \" \".join(new_words)\n\n    def compute_loss(self, batch, unlabeled_batch=None, wp=0, we=1):\n        \"\"\"\n        Compute SeFOSS loss according to Algorithm 2\n        \"\"\"\n        # Labeled data loss\n        input_ids = batch[\"input_ids\"].to(self.device)\n        attention_mask = batch[\"attention_mask\"].to(self.device)\n        labels = batch[\"label\"].to(self.device)\n\n        # Forward pass on labeled data\n        logits = self.backbone(input_ids, attention_mask)\n        labeled_loss = F.cross_entropy(logits, labels)\n\n        # Initialize total loss\n        total_loss = labeled_loss\n\n        # If we have unlabeled data\n        if unlabeled_batch is not None and len(unlabeled_batch[\"input_ids\"]) > 0:\n            # Get unlabeled data\n            u_input_ids = unlabeled_batch[\"input_ids\"].to(self.device)\n            u_attention_mask = unlabeled_batch[\"attention_mask\"].to(self.device)\n\n            # Get predictions on original unlabeled data\n            with torch.no_grad():\n                original_logits = self.backbone(u_input_ids, u_attention_mask)\n                original_probs = F.softmax(original_logits, dim=1)\n                original_features = self.backbone.get_features(\n                    u_input_ids, u_attention_mask\n                )\n\n                # Get confidence and pseudo-labels\n                max_probs, pseudo_labels = torch.max(original_probs, dim=1)\n\n            # Process weakly augmented data\n            mask_weak = max_probs >= self.tau_w\n\n            if torch.sum(mask_weak) > 0:\n                # Get weak augmentation loss\n                weak_logits = self.backbone(\n                    u_input_ids[mask_weak], u_attention_mask[mask_weak]\n                )\n                weak_probs = F.softmax(weak_logits, dim=1)\n                weak_loss = F.cross_entropy(weak_logits, pseudo_labels[mask_weak])\n                total_loss += self.lambda_w * weak_loss\n\n                # Get feature consistency loss for weakly augmented data\n                weak_features = self.backbone.get_features(\n                    u_input_ids[mask_weak], u_attention_mask[mask_weak]\n                )\n                feature_loss_weak = F.mse_loss(\n                    weak_features, original_features[mask_weak]\n                )\n                total_loss += self.lambda_f * feature_loss_weak\n\n            # Process strongly augmented data\n            mask_strong = max_probs >= self.tau_s\n\n            if torch.sum(mask_strong) > 0:\n                # Get strong augmentation loss\n                strong_logits = self.backbone(\n                    u_input_ids[mask_strong], u_attention_mask[mask_strong]\n                )\n                strong_loss = F.cross_entropy(strong_logits, pseudo_labels[mask_strong])\n                total_loss += self.lambda_s * strong_loss\n\n                # Get feature consistency loss for strongly augmented data\n                strong_features = self.backbone.get_features(\n                    u_input_ids[mask_strong], u_attention_mask[mask_strong]\n                )\n                feature_loss_strong = F.mse_loss(\n                    strong_features, original_features[mask_strong]\n                )\n                total_loss += self.lambda_f * feature_loss_strong\n\n            # Compute entropy loss (encouraging confident predictions)\n            entropy = -torch.sum(\n                original_probs * torch.log(original_probs + 1e-6), dim=1\n            ).mean()\n            total_loss += self.lambda_e * entropy * we\n\n        # Add pretraining weight if needed\n        if wp > 0:\n            total_loss = total_loss * wp\n\n        return total_loss\n\n    def compute_thresholds(self, labeled_loader):\n        \"\"\"Compute confidence thresholds based on labeled data\"\"\"\n        self.backbone.eval()\n        confidences = []\n\n        with torch.no_grad():\n            for batch in labeled_loader:\n                input_ids = batch[\"input_ids\"].to(self.device)\n                attention_mask = batch[\"attention_mask\"].to(self.device)\n\n                logits = self.backbone(input_ids, attention_mask)\n                probs = F.softmax(logits, dim=1)\n                max_probs, _ = torch.max(probs, dim=1)\n\n                confidences.extend(max_probs.cpu().numpy())\n\n        confidences = np.array(confidences)\n\n        # Set thresholds based on percentiles\n        self.tau_d = np.percentile(confidences, 50)  # median\n        self.tau_s = np.percentile(confidences, 75)  # 75th percentile\n        self.tau_w = np.percentile(confidences, 25)  # 25th percentile\n\n        print(\n            f\"Computed thresholds: tau_d={self.tau_d:.4f}, tau_s={self.tau_s:.4f}, tau_w={self.tau_w:.4f}\"\n        )\n\n        self.backbone.train()\n        return self.tau_d, self.tau_s, self.tau_w\n\n    def train(\n        self,\n        labeled_dataset,\n        unlabeled_dataset,\n        val_dataset,\n        test_dataset,\n        num_epochs=10,\n        pretraining_epochs=5,\n    ):\n        \"\"\"\n        Train the SeFOSS model according to Algorithm 1\n        \"\"\"\n        # Create dataloaders\n        labeled_loader = DataLoader(\n            labeled_dataset, batch_size=self.batch_size, shuffle=True\n        )\n        unlabeled_loader = DataLoader(\n            unlabeled_dataset, batch_size=self.batch_size, shuffle=True\n        )\n        val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n        test_loader = DataLoader(test_dataset, batch_size=self.batch_size)\n\n        # For tracking metrics\n        train_losses = []\n        val_losses = []\n        val_accuracies = []\n        best_val_accuracy = 0.0\n\n        # Pretraining loop (lines 1-5 in Algorithm 1)\n        print(\"Starting pretraining phase...\")\n        for epoch in range(pretraining_epochs):\n            self.backbone.train()\n            epoch_loss = 0.0\n\n            for batch in tqdm(\n                labeled_loader, desc=f\"Pretraining Epoch {epoch+1}/{pretraining_epochs}\"\n            ):\n                self.optimizer.zero_grad()\n\n                # Compute loss (with wp=1, we=0 to ignore entropy minimization)\n                loss = self.compute_loss(batch, wp=1, we=0)\n\n                loss.backward()\n                self.optimizer.step()\n\n                epoch_loss += loss.item()\n\n            avg_epoch_loss = epoch_loss / len(labeled_loader)\n            train_losses.append(avg_epoch_loss)\n\n            print(\n                f\"Pretraining Epoch {epoch+1}/{pretraining_epochs}, Loss: {avg_epoch_loss:.4f}\"\n            )\n\n            # Validate\n            val_loss, val_accuracy = self.evaluate(val_loader)\n            val_losses.append(val_loss)\n            val_accuracies.append(val_accuracy)\n\n            print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n\n            # Save best model\n            if val_accuracy > best_val_accuracy:\n                best_val_accuracy = val_accuracy\n                self.save_model(\"best_pretrained_model.pkl\")\n\n        # Compute confidence thresholds (lines 6-10 in Algorithm 1)\n        print(\"Computing confidence thresholds...\")\n        self.compute_thresholds(labeled_loader)\n\n        # Load best pretrained model\n        self.load_model(\"best_pretrained_model.pkl\")\n\n        # Training loop with SeFOSS (lines 11-15 in Algorithm 1)\n        print(\"Starting SeFOSS training phase...\")\n        for epoch in range(num_epochs):\n            self.backbone.train()\n            epoch_loss = 0.0\n\n            # Create an iterable for the unlabeled data\n            unlabeled_iter = iter(unlabeled_loader)\n\n            for batch in tqdm(\n                labeled_loader, desc=f\"SeFOSS Training Epoch {epoch+1}/{num_epochs}\"\n            ):\n                self.optimizer.zero_grad()\n\n                # Get unlabeled batch\n                try:\n                    unlabeled_batch = next(unlabeled_iter)\n                except StopIteration:\n                    unlabeled_iter = iter(unlabeled_loader)\n                    unlabeled_batch = next(unlabeled_iter)\n\n                # Compute loss with SeFOSS\n                loss = self.compute_loss(batch, unlabeled_batch, wp=0, we=1)\n\n                loss.backward()\n                self.optimizer.step()\n\n                epoch_loss += loss.item()\n\n            avg_epoch_loss = epoch_loss / len(labeled_loader)\n            train_losses.append(avg_epoch_loss)\n\n            print(\n                f\"SeFOSS Training Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}\"\n            )\n\n            # Validate\n            val_loss, val_accuracy = self.evaluate(val_loader)\n            val_losses.append(val_loss)\n            val_accuracies.append(val_accuracy)\n\n            print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n\n            # Save best model\n            if val_accuracy > best_val_accuracy:\n                best_val_accuracy = val_accuracy\n                self.save_model(\"best_model.pkl\")\n\n        # Load best model for final evaluation\n        self.load_model(\"best_model.pkl\")\n\n        # Final evaluation on test set\n        test_loss, test_accuracy = self.evaluate(test_loader)\n        print(f\"Final Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\")\n\n        # Save training history\n        history = {\n            \"train_losses\": train_losses,\n            \"val_losses\": val_losses,\n            \"val_accuracies\": val_accuracies,\n            \"final_test_loss\": test_loss,\n            \"final_test_accuracy\": test_accuracy,\n        }\n\n        with open(os.path.join(self.save_dir, \"training_history.pkl\"), \"wb\") as f:\n            pickle.dump(history, f)\n\n        # Plot and save learning curves\n        self.plot_learning_curves(train_losses, val_losses, val_accuracies)\n\n        # Detailed evaluation and save metrics\n        self.detailed_evaluation(test_loader)\n\n        return self.backbone\n\n    def evaluate(self, dataloader):\n        \"\"\"Evaluate the model on given dataloader\"\"\"\n        self.backbone.eval()\n        total_loss = 0\n        all_preds = []\n        all_labels = []\n\n        with torch.no_grad():\n            for batch in dataloader:\n                input_ids = batch[\"input_ids\"].to(self.device)\n                attention_mask = batch[\"attention_mask\"].to(self.device)\n                labels = batch[\"label\"].to(self.device)\n\n                # Forward pass\n                logits = self.backbone(input_ids, attention_mask)\n                loss = F.cross_entropy(logits, labels)\n\n                # Get predictions\n                _, preds = torch.max(logits, 1)\n\n                total_loss += loss.item()\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n\n        # Calculate metrics\n        accuracy = accuracy_score(all_labels, all_preds)\n        avg_loss = total_loss / len(dataloader)\n\n        return avg_loss, accuracy\n\n    def detailed_evaluation(self, dataloader):\n        \"\"\"Detailed evaluation with multiple metrics\"\"\"\n        self.backbone.eval()\n        all_preds = []\n        all_labels = []\n\n        with torch.no_grad():\n            for batch in dataloader:\n                input_ids = batch[\"input_ids\"].to(self.device)\n                attention_mask = batch[\"attention_mask\"].to(self.device)\n                labels = batch[\"label\"].to(self.device)\n\n                # Forward pass\n                logits = self.backbone(input_ids, attention_mask)\n\n                # Get predictions\n                _, preds = torch.max(logits, 1)\n\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n\n        # Calculate metrics\n        accuracy = accuracy_score(all_labels, all_preds)\n        precision_macro = precision_score(all_labels, all_preds, average=\"macro\")\n        recall_macro = recall_score(all_labels, all_preds, average=\"macro\")\n        f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n\n        # Per-class metrics\n        precision_per_class = precision_score(all_labels, all_preds, average=None)\n        recall_per_class = recall_score(all_labels, all_preds, average=None)\n        f1_per_class = f1_score(all_labels, all_preds, average=None)\n\n        # Save metrics to CSV\n        metrics = {\n            \"Metric\": [\n                \"Accuracy\",\n                \"Precision (Macro)\",\n                \"Recall (Macro)\",\n                \"F1 Score (Macro)\",\n            ],\n            \"Value\": [accuracy, precision_macro, recall_macro, f1_macro],\n        }\n        metrics_df = pd.DataFrame(metrics)\n        metrics_df.to_csv(os.path.join(self.save_dir, \"test_metrics.csv\"), index=False)\n\n        # Save per-class metrics\n        class_metrics = {\n            \"Class\": list(range(self.num_classes)),\n            \"Precision\": precision_per_class,\n            \"Recall\": recall_per_class,\n            \"F1 Score\": f1_per_class,\n        }\n        class_metrics_df = pd.DataFrame(class_metrics)\n        class_metrics_df.to_csv(\n            os.path.join(self.save_dir, \"per_class_metrics.csv\"), index=False\n        )\n\n        # Confusion matrix\n        cm = confusion_matrix(all_labels, all_preds)\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n        plt.title(\"Confusion Matrix\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.savefig(os.path.join(self.save_dir, \"confusion_matrix.png\"))\n        plt.close()\n\n        print(f\"Detailed evaluation saved to {self.save_dir}\")\n        print(f\"Accuracy: {accuracy:.4f}\")\n        print(f\"Precision (Macro): {precision_macro:.4f}\")\n        print(f\"Recall (Macro): {recall_macro:.4f}\")\n        print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n\n        return {\n            \"accuracy\": accuracy,\n            \"precision\": precision_macro,\n            \"recall\": recall_macro,\n            \"f1\": f1_macro,\n        }\n\n    def plot_learning_curves(self, train_losses, val_losses, val_accuracies):\n        \"\"\"Plot and save learning curves\"\"\"\n        # Create figure with two subplots\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\n        # Plot losses\n        ax1.plot(train_losses, label=\"Training Loss\")\n        ax1.plot(val_losses, label=\"Validation Loss\")\n        ax1.set_xlabel(\"Epochs\")\n        ax1.set_ylabel(\"Loss\")\n        ax1.set_title(\"Training and Validation Loss\")\n        ax1.legend()\n        ax1.grid(True)\n\n        # Plot accuracy\n        ax2.plot(val_accuracies, label=\"Validation Accuracy\", color=\"green\")\n        ax2.set_xlabel(\"Epochs\")\n        ax2.set_ylabel(\"Accuracy\")\n        ax2.set_title(\"Validation Accuracy\")\n        ax2.legend()\n        ax2.grid(True)\n\n        # Save figure\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.save_dir, \"learning_curves.png\"))\n        plt.close()\n\n    def save_model(self, filename):\n        \"\"\"Save model parameters\"\"\"\n        filepath = os.path.join(self.save_dir, filename)\n        torch.save(self.backbone.state_dict(), filepath)\n        print(f\"Model saved to {filepath}\")\n\n    def load_model(self, filename):\n        \"\"\"Load model parameters\"\"\"\n        filepath = os.path.join(self.save_dir, filename)\n        if os.path.exists(filepath):\n            self.backbone.load_state_dict(torch.load(filepath))\n            print(f\"Model loaded from {filepath}\")\n            return True\n        else:\n            print(f\"No model found at {filepath}\")\n            return False\n\n\ndef analyze_different_labeled_amounts(\n    train_dataset,\n    val_dataset,\n    test_dataset,\n    labeled_amounts,\n    num_epochs=5,\n    pretraining_epochs=2,\n):\n    \"\"\"\n    Analyze model performance with different amounts of labeled data\n\n    Args:\n        train_dataset: Full training dataset\n        val_dataset: Validation dataset\n        test_dataset: Test dataset\n        labeled_amounts: List of integers for number of samples per class\n    \"\"\"\n    results = []\n\n    for n_labeled in labeled_amounts:\n        print(f\"\\n\\n----- Training with {n_labeled} labeled samples per class -----\\n\")\n\n        # Create directory for this run\n        run_dir = os.path.join(SAVE_DIR, f\"labeled_{n_labeled}\")\n        os.makedirs(run_dir, exist_ok=True)\n\n        # Get balanced subset for labeled data\n        labeled_dataset = train_dataset.get_balanced_subset(n_labeled)\n\n        # The rest becomes unlabeled data\n        labeled_indices = set()\n        for i, label in enumerate(labeled_dataset.labels):\n            labeled_indices.add(i)\n\n        unlabeled_data = []\n        unlabeled_labels = (\n            []\n        )  # We keep the labels for evaluation, but don't use them in training\n\n        for i in range(len(train_dataset)):\n            if i not in labeled_indices:\n                unlabeled_data.append(train_dataset.get_text(i))\n                unlabeled_labels.append(train_dataset.labels[i])\n\n        # Create unlabeled dataset\n        unlabeled_dataset = YahooAnswersDataset(\n            train_dataset.root_dir,\n            max_files=0,\n            tokenizer=train_dataset.tokenizer,\n            max_length=train_dataset.max_length,\n        )\n        unlabeled_dataset.data = unlabeled_data\n        unlabeled_dataset.labels = unlabeled_labels\n\n        print(f\"Created labeled dataset with {len(labeled_dataset)} samples\")\n        print(f\"Created unlabeled dataset with {len(unlabeled_dataset)} samples\")\n\n        # Initialize and train SeFOSS\n        sefoss = SeFOSS(num_classes=10, device=device)\n        sefoss.save_dir = run_dir\n\n        # Train the model\n        sefoss.train(\n            labeled_dataset,\n            unlabeled_dataset,\n            val_dataset,\n            test_dataset,\n            num_epochs=num_epochs,\n            pretraining_epochs=pretraining_epochs,\n        )\n\n        # Get final metrics\n        _, accuracy = sefoss.evaluate(DataLoader(test_dataset, batch_size=16))\n        metrics = sefoss.detailed_evaluation(DataLoader(test_dataset, batch_size=16))\n\n        # Save results\n        results.append(\n            {\n                \"n_labeled\": n_labeled,\n                \"accuracy\": metrics[\"accuracy\"],\n                \"precision\": metrics[\"precision\"],\n                \"recall\": metrics[\"recall\"],\n                \"f1\": metrics[\"f1\"],\n            }\n        )\n\n    # Create and save results dataframe\n    results_df = pd.DataFrame(results)\n    results_df.to_csv(os.path.join(SAVE_DIR, \"labeled_amount_results.csv\"), index=False)\n\n    # Plot results\n    plt.figure(figsize=(10, 6))\n    plt.plot(\n        results_df[\"n_labeled\"], results_df[\"accuracy\"], marker=\"o\", label=\"Accuracy\"\n    )\n    plt.plot(results_df[\"n_labeled\"], results_df[\"f1\"], marker=\"s\", label=\"F1 Score\")\n    plt.xlabel(\"Number of Labeled Samples per Class\")\n    plt.ylabel(\"Score\")\n    plt.title(\"Performance vs. Amount of Labeled Data\")\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(os.path.join(SAVE_DIR, \"labeled_amount_performance.png\"))\n    plt.close()\n\n    return results_df\n\n\ndef main():\n    print(\"Starting SeFOSS training for Yahoo Answers dataset\")\n\n    # Load tokenizer\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n    # Load and preprocess data\n    print(\"Loading datasets...\")\n    train_dataset = YahooAnswersDataset(\n        \"/kaggle/input/yahooanswerssplited/text/train\",\n        max_files=1000,\n        tokenizer=tokenizer,\n    )\n    val_dataset = YahooAnswersDataset(\n        \"/kaggle/input/yahooanswerssplited/text/val\",\n        max_files=1000,\n        tokenizer=tokenizer,\n    )\n    test_dataset = YahooAnswersDataset(\n        \"/kaggle/input/yahooanswerssplited/text/test\",\n        max_files=1000,\n        tokenizer=tokenizer,\n    )\n\n    # Analyze different labeled amounts\n    labeled_amounts = [40, 150, 300, 500]\n    result_df = analyze_different_labeled_amounts(\n        train_dataset,\n        val_dataset,\n        test_dataset,\n        labeled_amounts,\n        num_epochs=3,\n        pretraining_epochs=2,\n    )\n\n    # Print summary of results\n    print(\"\\n=== Summary of Results ===\")\n    print(result_df)\n\n    # Train the final model with all labeled data\n    print(\"\\n=== Training final model with full labeled data ===\")\n    final_model_dir = os.path.join(SAVE_DIR, \"final_model\")\n    os.makedirs(final_model_dir, exist_ok=True)\n\n    # Create full dataset split\n    n_labeled = 500  # Use maximum labeled samples per class\n    labeled_dataset = train_dataset.get_balanced_subset(n_labeled)\n\n    # Create unlabeled dataset from remaining samples\n    labeled_indices = set()\n    for i, label in enumerate(labeled_dataset.labels):\n        labeled_indices.add(i)\n\n    unlabeled_data = []\n    unlabeled_labels = []\n    for i in range(len(train_dataset)):\n        if i not in labeled_indices:\n            unlabeled_data.append(train_dataset.get_text(i))\n            unlabeled_labels.append(train_dataset.labels[i])\n\n    unlabeled_dataset = YahooAnswersDataset(\n        train_dataset.root_dir,\n        max_files=0,\n        tokenizer=train_dataset.tokenizer,\n        max_length=train_dataset.max_length,\n    )\n    unlabeled_dataset.data = unlabeled_data\n    unlabeled_dataset.labels = unlabeled_labels\n\n    print(f\"Final model: Using {len(labeled_dataset)} labeled samples\")\n    print(f\"Final model: Using {len(unlabeled_dataset)} unlabeled samples\")\n\n    # Initialize SeFOSS with final settings\n    final_sefoss = SeFOSS(num_classes=10, device=device)\n    final_sefoss.save_dir = final_model_dir\n\n    # Train final model with full pretraining and training cycles\n    final_sefoss.train(\n        labeled_dataset,\n        unlabeled_dataset,\n        val_dataset,\n        test_dataset,\n        num_epochs=5,\n        pretraining_epochs=3,\n    )\n\n    # Generate class distribution analysis\n    class_distribution = Counter(train_dataset.labels)\n    plt.figure(figsize=(10, 6))\n    plt.bar(range(10), [class_distribution.get(i, 0) for i in range(10)])\n    plt.xlabel(\"Class\")\n    plt.ylabel(\"Number of Samples\")\n    plt.title(\"Class Distribution in Yahoo Answers Dataset\")\n    plt.xticks(range(10))\n    plt.grid(axis=\"y\")\n    plt.savefig(os.path.join(SAVE_DIR, \"class_distribution.png\"))\n    plt.close()\n\n    # Generate a summary report\n    summary = {\n        \"Dataset\": \"Yahoo Answers\",\n        \"Total Samples\": len(train_dataset),\n        \"Classes\": 10,\n        \"Best Accuracy (500 labeled)\": result_df.loc[\n            result_df[\"n_labeled\"] == 500, \"accuracy\"\n        ].values[0],\n        \"Best F1 Score (500 labeled)\": result_df.loc[\n            result_df[\"n_labeled\"] == 500, \"f1\"\n        ].values[0],\n        \"Accuracy with 40 labeled\": result_df.loc[\n            result_df[\"n_labeled\"] == 40, \"accuracy\"\n        ].values[0],\n        \"Model\": \"BERT + SeFOSS\",\n    }\n\n    summary_df = pd.DataFrame(list(summary.items()), columns=[\"Metric\", \"Value\"])\n    summary_df.to_csv(os.path.join(SAVE_DIR, \"experiment_summary.csv\"), index=False)\n\n    print(\"\\n=== Experiment complete ===\")\n    print(f\"All results saved to {SAVE_DIR}\")\n\n    return result_df\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T16:56:03.240578Z","iopub.execute_input":"2025-04-08T16:56:03.241044Z","iopub.status.idle":"2025-04-08T18:39:36.560503Z","shell.execute_reply.started":"2025-04-08T16:56:03.241002Z","shell.execute_reply":"2025-04-08T18:39:36.559335Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nStarting SeFOSS training for Yahoo Answers dataset\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c57924dbd0943eeb2c3fb2a1d634f83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70d1368bc6b54217ba9bc4db69286dc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"222a8a5a7e42472fb361bfcae34be442"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33da5e2a90304e3f9a81df424c0fc573"}},"metadata":{}},{"name":"stdout","text":"Loading datasets...\nLoaded 10000 files from /kaggle/input/yahooanswerssplited/text/train\nFiles per class: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}\nLoaded 10000 files from /kaggle/input/yahooanswerssplited/text/val\nFiles per class: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}\nLoaded 10000 files from /kaggle/input/yahooanswerssplited/text/test\nFiles per class: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}\n\n\n----- Training with 40 labeled samples per class -----\n\nLoaded 0 files from /kaggle/input/yahooanswerssplited/text/train\nFiles per class: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\nLoaded 0 files from /kaggle/input/yahooanswerssplited/text/train\nFiles per class: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\nCreated labeled dataset with 400 samples\nCreated unlabeled dataset with 9600 samples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b766355acc474dc684298a49f236ab44"}},"metadata":{}},{"name":"stdout","text":"Starting pretraining phase...\n","output_type":"stream"},{"name":"stderr","text":"Pretraining Epoch 1/2: 100%|██████████| 25/25 [00:06<00:00,  3.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pretraining Epoch 1/2, Loss: 2.3245\nValidation Loss: 2.2596, Accuracy: 0.1362\nModel saved to SEFOSS_TEXTData/labeled_40/best_pretrained_model.pkl\n","output_type":"stream"},{"name":"stderr","text":"Pretraining Epoch 2/2: 100%|██████████| 25/25 [00:06<00:00,  4.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pretraining Epoch 2/2, Loss: 2.1119\nValidation Loss: 1.9733, Accuracy: 0.4352\nModel saved to SEFOSS_TEXTData/labeled_40/best_pretrained_model.pkl\nComputing confidence thresholds...\nComputed thresholds: tau_d=0.1806, tau_s=0.2084, tau_w=0.1613\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-41f564467cab>:655: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.backbone.load_state_dict(torch.load(filepath))\n","output_type":"stream"},{"name":"stdout","text":"Model loaded from SEFOSS_TEXTData/labeled_40/best_pretrained_model.pkl\nStarting SeFOSS training phase...\n","output_type":"stream"},{"name":"stderr","text":"SeFOSS Training Epoch 1/3: 100%|██████████| 25/25 [00:19<00:00,  1.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"SeFOSS Training Epoch 1/3, Loss: 6.0399\nValidation Loss: 2.3942, Accuracy: 0.2330\n","output_type":"stream"},{"name":"stderr","text":"SeFOSS Training Epoch 2/3: 100%|██████████| 25/25 [00:21<00:00,  1.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"SeFOSS Training Epoch 2/3, Loss: 4.4465\nValidation Loss: 2.0625, Accuracy: 0.3294\n","output_type":"stream"},{"name":"stderr","text":"SeFOSS Training Epoch 3/3: 100%|██████████| 25/25 [00:21<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"SeFOSS Training Epoch 3/3, Loss: 3.8956\nValidation Loss: 1.9268, Accuracy: 0.3977\nNo model found at SEFOSS_TEXTData/labeled_40/best_model.pkl\nFinal Test Loss: 1.9205, Accuracy: 0.3974\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Detailed evaluation saved to SEFOSS_TEXTData/labeled_40\nAccuracy: 0.3974\nPrecision (Macro): 0.4580\nRecall (Macro): 0.3974\nF1 Score (Macro): 0.2903\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Detailed evaluation saved to SEFOSS_TEXTData/labeled_40\nAccuracy: 0.3974\nPrecision (Macro): 0.4580\nRecall (Macro): 0.3974\nF1 Score (Macro): 0.2903\n\n\n----- Training with 150 labeled samples per class -----\n\nLoaded 0 files from /kaggle/input/yahooanswerssplited/text/train\nFiles per class: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\nLoaded 0 files from /kaggle/input/yahooanswerssplited/text/train\nFiles per class: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\nCreated labeled dataset with 1500 samples\nCreated unlabeled dataset with 8500 samples\nStarting pretraining phase...\n","output_type":"stream"},{"name":"stderr","text":"Pretraining Epoch 1/2: 100%|██████████| 94/94 [00:22<00:00,  4.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pretraining Epoch 1/2, Loss: 2.0773\nValidation Loss: 1.6123, Accuracy: 0.6204\nModel saved to SEFOSS_TEXTData/labeled_150/best_pretrained_model.pkl\n","output_type":"stream"},{"name":"stderr","text":"Pretraining Epoch 2/2: 100%|██████████| 94/94 [00:22<00:00,  4.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pretraining Epoch 2/2, Loss: 1.2711\nValidation Loss: 1.1099, Accuracy: 0.6785\nModel saved to SEFOSS_TEXTData/labeled_150/best_pretrained_model.pkl\nComputing confidence thresholds...\nComputed thresholds: tau_d=0.6312, tau_s=0.7875, tau_w=0.4588\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-41f564467cab>:655: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.backbone.load_state_dict(torch.load(filepath))\n","output_type":"stream"},{"name":"stdout","text":"Model loaded from SEFOSS_TEXTData/labeled_150/best_pretrained_model.pkl\nStarting SeFOSS training phase...\n","output_type":"stream"},{"name":"stderr","text":"SeFOSS Training Epoch 1/3: 100%|██████████| 94/94 [01:09<00:00,  1.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"SeFOSS Training Epoch 1/3, Loss: 2.2389\nValidation Loss: 1.0642, Accuracy: 0.6858\nModel saved to SEFOSS_TEXTData/labeled_150/best_model.pkl\n","output_type":"stream"},{"name":"stderr","text":"SeFOSS Training Epoch 2/3: 100%|██████████| 94/94 [01:14<00:00,  1.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"SeFOSS Training Epoch 2/3, Loss: 1.6055\nValidation Loss: 1.2409, Accuracy: 0.6817\n","output_type":"stream"},{"name":"stderr","text":"SeFOSS Training Epoch 3/3: 100%|██████████| 94/94 [01:15<00:00,  1.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"SeFOSS Training Epoch 3/3, Loss: 1.2736\nValidation Loss: 1.3034, Accuracy: 0.6802\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-41f564467cab>:655: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.backbone.load_state_dict(torch.load(filepath))\n","output_type":"stream"},{"name":"stdout","text":"Model loaded from SEFOSS_TEXTData/labeled_150/best_model.pkl\nFinal Test Loss: 1.0516, Accuracy: 0.6905\nDetailed evaluation saved to SEFOSS_TEXTData/labeled_150\nAccuracy: 0.6905\nPrecision (Macro): 0.6945\nRecall (Macro): 0.6905\nF1 Score (Macro): 0.6880\nDetailed evaluation saved to SEFOSS_TEXTData/labeled_150\nAccuracy: 0.6905\nPrecision (Macro): 0.6945\nRecall (Macro): 0.6905\nF1 Score (Macro): 0.6880\n\n\n----- Training with 300 labeled samples per class -----\n\nLoaded 0 files from /kaggle/input/yahooanswerssplited/text/train\nFiles per class: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\nLoaded 0 files from /kaggle/input/yahooanswerssplited/text/train\nFiles per class: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\nCreated labeled dataset with 3000 samples\nCreated unlabeled dataset with 7000 samples\nStarting pretraining phase...\n","output_type":"stream"},{"name":"stderr","text":"Pretraining Epoch 1/2: 100%|██████████| 188/188 [00:44<00:00,  4.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pretraining Epoch 1/2, Loss: 1.7246\nValidation Loss: 1.0995, Accuracy: 0.6792\nModel saved to SEFOSS_TEXTData/labeled_300/best_pretrained_model.pkl\n","output_type":"stream"},{"name":"stderr","text":"Pretraining Epoch 2/2: 100%|██████████| 188/188 [00:44<00:00,  4.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pretraining Epoch 2/2, Loss: 0.9210\nValidation Loss: 0.9581, Accuracy: 0.7104\nModel saved to SEFOSS_TEXTData/labeled_300/best_pretrained_model.pkl\nComputing confidence thresholds...\nComputed thresholds: tau_d=0.8188, tau_s=0.8978, tau_w=0.6701\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-41f564467cab>:655: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.backbone.load_state_dict(torch.load(filepath))\n","output_type":"stream"},{"name":"stdout","text":"Model loaded from SEFOSS_TEXTData/labeled_300/best_pretrained_model.pkl\nStarting SeFOSS training phase...\n","output_type":"stream"},{"name":"stderr","text":"SeFOSS Training Epoch 1/3: 100%|██████████| 188/188 [02:15<00:00,  1.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"SeFOSS Training Epoch 1/3, Loss: 1.5148\nValidation Loss: 1.1538, Accuracy: 0.6955\n","output_type":"stream"},{"name":"stderr","text":"SeFOSS Training Epoch 2/3: 100%|██████████| 188/188 [02:25<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"SeFOSS Training Epoch 2/3, Loss: 1.1113\nValidation Loss: 1.2952, Accuracy: 0.6875\n","output_type":"stream"},{"name":"stderr","text":"SeFOSS Training Epoch 3/3: 100%|██████████| 188/188 [02:28<00:00,  1.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"SeFOSS Training Epoch 3/3, Loss: 0.8851\nValidation Loss: 1.3359, Accuracy: 0.6906\nNo model found at SEFOSS_TEXTData/labeled_300/best_model.pkl\nFinal Test Loss: 1.3231, Accuracy: 0.6978\nDetailed evaluation saved to SEFOSS_TEXTData/labeled_300\nAccuracy: 0.6978\nPrecision (Macro): 0.7050\nRecall (Macro): 0.6978\nF1 Score (Macro): 0.6960\nDetailed evaluation saved to SEFOSS_TEXTData/labeled_300\nAccuracy: 0.6978\nPrecision (Macro): 0.7050\nRecall (Macro): 0.6978\nF1 Score (Macro): 0.6960\n\n\n----- Training with 500 labeled samples per class -----\n\nLoaded 0 files from /kaggle/input/yahooanswerssplited/text/train\nFiles per class: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\nLoaded 0 files from /kaggle/input/yahooanswerssplited/text/train\nFiles per class: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\nCreated labeled dataset with 5000 samples\nCreated unlabeled dataset with 5000 samples\nStarting pretraining phase...\n","output_type":"stream"},{"name":"stderr","text":"Pretraining Epoch 1/2: 100%|██████████| 313/313 [01:14<00:00,  4.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pretraining Epoch 1/2, Loss: 1.4556\nValidation Loss: 0.9699, Accuracy: 0.7030\nModel saved to SEFOSS_TEXTData/labeled_500/best_pretrained_model.pkl\n","output_type":"stream"},{"name":"stderr","text":"Pretraining Epoch 2/2: 100%|██████████| 313/313 [01:14<00:00,  4.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pretraining Epoch 2/2, Loss: 0.7855\nValidation Loss: 0.9397, Accuracy: 0.7115\nModel saved to SEFOSS_TEXTData/labeled_500/best_pretrained_model.pkl\nComputing confidence thresholds...\nComputed thresholds: tau_d=0.8744, tau_s=0.9307, tau_w=0.7163\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-41f564467cab>:655: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.backbone.load_state_dict(torch.load(filepath))\n","output_type":"stream"},{"name":"stdout","text":"Model loaded from SEFOSS_TEXTData/labeled_500/best_pretrained_model.pkl\nStarting SeFOSS training phase...\n","output_type":"stream"},{"name":"stderr","text":"SeFOSS Training Epoch 1/3: 100%|██████████| 313/313 [03:54<00:00,  1.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"SeFOSS Training Epoch 1/3, Loss: 1.1842\nValidation Loss: 1.1022, Accuracy: 0.7039\n","output_type":"stream"},{"name":"stderr","text":"SeFOSS Training Epoch 2/3: 100%|██████████| 313/313 [04:07<00:00,  1.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"SeFOSS Training Epoch 2/3, Loss: 0.8141\nValidation Loss: 1.2111, Accuracy: 0.7089\n","output_type":"stream"},{"name":"stderr","text":"SeFOSS Training Epoch 3/3: 100%|██████████| 313/313 [04:13<00:00,  1.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"SeFOSS Training Epoch 3/3, Loss: 0.6000\nValidation Loss: 1.3236, Accuracy: 0.7000\nNo model found at SEFOSS_TEXTData/labeled_500/best_model.pkl\nFinal Test Loss: 1.3202, Accuracy: 0.7020\nDetailed evaluation saved to SEFOSS_TEXTData/labeled_500\nAccuracy: 0.7020\nPrecision (Macro): 0.7057\nRecall (Macro): 0.7020\nF1 Score (Macro): 0.6993\nDetailed evaluation saved to SEFOSS_TEXTData/labeled_500\nAccuracy: 0.7020\nPrecision (Macro): 0.7057\nRecall (Macro): 0.7020\nF1 Score (Macro): 0.6993\n\n=== Summary of Results ===\n   n_labeled  accuracy  precision  recall        f1\n0         40    0.3974   0.457971  0.3974  0.290291\n1        150    0.6905   0.694454  0.6905  0.688038\n2        300    0.6978   0.705037  0.6978  0.696024\n3        500    0.7020   0.705710  0.7020  0.699283\n\n=== Training final model with full labeled data ===\nLoaded 0 files from /kaggle/input/yahooanswerssplited/text/train\nFiles per class: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\nLoaded 0 files from /kaggle/input/yahooanswerssplited/text/train\nFiles per class: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\nFinal model: Using 5000 labeled samples\nFinal model: Using 5000 unlabeled samples\nStarting pretraining phase...\n","output_type":"stream"},{"name":"stderr","text":"Pretraining Epoch 1/3: 100%|██████████| 313/313 [01:13<00:00,  4.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pretraining Epoch 1/3, Loss: 1.3568\nValidation Loss: 0.9755, Accuracy: 0.6962\nModel saved to SEFOSS_TEXTData/final_model/best_pretrained_model.pkl\n","output_type":"stream"},{"name":"stderr","text":"Pretraining Epoch 2/3: 100%|██████████| 313/313 [01:14<00:00,  4.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pretraining Epoch 2/3, Loss: 0.7641\nValidation Loss: 0.9567, Accuracy: 0.7100\nModel saved to SEFOSS_TEXTData/final_model/best_pretrained_model.pkl\n","output_type":"stream"},{"name":"stderr","text":"Pretraining Epoch 3/3: 100%|██████████| 313/313 [01:14<00:00,  4.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Pretraining Epoch 3/3, Loss: 0.5075\nValidation Loss: 1.0208, Accuracy: 0.7054\nComputing confidence thresholds...\nComputed thresholds: tau_d=0.9437, tau_s=0.9669, tau_w=0.8860\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-41f564467cab>:655: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.backbone.load_state_dict(torch.load(filepath))\n","output_type":"stream"},{"name":"stdout","text":"Model loaded from SEFOSS_TEXTData/final_model/best_pretrained_model.pkl\nStarting SeFOSS training phase...\n","output_type":"stream"},{"name":"stderr","text":"SeFOSS Training Epoch 1/5: 100%|██████████| 313/313 [03:30<00:00,  1.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"SeFOSS Training Epoch 1/5, Loss: 1.1469\nValidation Loss: 1.0810, Accuracy: 0.6957\n","output_type":"stream"},{"name":"stderr","text":"SeFOSS Training Epoch 2/5: 100%|██████████| 313/313 [03:51<00:00,  1.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"SeFOSS Training Epoch 2/5, Loss: 0.8273\nValidation Loss: 1.2112, Accuracy: 0.7050\n","output_type":"stream"},{"name":"stderr","text":"SeFOSS Training Epoch 3/5: 100%|██████████| 313/313 [04:03<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"SeFOSS Training Epoch 3/5, Loss: 0.6053\nValidation Loss: 1.3099, Accuracy: 0.6981\n","output_type":"stream"},{"name":"stderr","text":"SeFOSS Training Epoch 4/5: 100%|██████████| 313/313 [04:06<00:00,  1.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"SeFOSS Training Epoch 4/5, Loss: 0.5007\nValidation Loss: 1.4287, Accuracy: 0.6953\n","output_type":"stream"},{"name":"stderr","text":"SeFOSS Training Epoch 5/5: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"SeFOSS Training Epoch 5/5, Loss: 0.3912\nValidation Loss: 1.4593, Accuracy: 0.6956\nNo model found at SEFOSS_TEXTData/final_model/best_model.pkl\nFinal Test Loss: 1.4733, Accuracy: 0.6953\nDetailed evaluation saved to SEFOSS_TEXTData/final_model\nAccuracy: 0.6953\nPrecision (Macro): 0.6984\nRecall (Macro): 0.6953\nF1 Score (Macro): 0.6956\n\n=== Experiment complete ===\nAll results saved to SEFOSS_TEXTData\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport zipfile\n\n# Define the directory to zip and the output zip file path\ndir_to_zip = \"/kaggle/working/SEFOSS_TEXTData\"\nzip_path = \"/kaggle/working/SEFOSS_TEXTData.zip\"\n\n# Create a zip file\nwith zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for root, dirs, files in os.walk(dir_to_zip):\n        for file in files:\n            file_path = os.path.join(root, file)\n            # Add file to zip, maintaining relative path\n            zipf.write(file_path, os.path.relpath(file_path, dir_to_zip))\n\nprint(f\"Zipped output directory to: {zip_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T18:40:59.862604Z","iopub.execute_input":"2025-04-08T18:40:59.862908Z","iopub.status.idle":"2025-04-08T18:43:21.034288Z","shell.execute_reply.started":"2025-04-08T18:40:59.862881Z","shell.execute_reply":"2025-04-08T18:43:21.033483Z"}},"outputs":[{"name":"stdout","text":"Zipped output directory to: /kaggle/working/SEFOSS_TEXTData.zip\n","output_type":"stream"}],"execution_count":2}]}