{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11362983,"sourceType":"datasetVersion","datasetId":7112073}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset, random_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    confusion_matrix,\n)\nimport librosa\nimport torchaudio\nfrom torchaudio import transforms\nimport random\nfrom collections import defaultdict, Counter\nimport torchvision.models as models\nimport warnings\nimport pickle\nimport time\nimport shutil\nfrom pathlib import Path\n\n# Ignore warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set random seeds for reproducibility\nSEED = 42\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Create output directory\noutput_dir = \"SEFOSS_AudioData\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Audio constants\nSAMPLE_RATE = 44100\nDURATION = 5  # seconds\nNUM_SAMPLES = SAMPLE_RATE * DURATION\nN_MELS = 128\nN_FFT = 1024\nHOP_LENGTH = 512\nFMIN = 20\nFMAX = SAMPLE_RATE // 2\n\n# Training constants\nBATCH_SIZE = 32\nNUM_EPOCHS = 50\nLEARNING_RATE = 0.001\nNUM_CLASSES = 41  # Based on FSDKaggle2018 dataset\n\n# Define paths\nTRAIN_AUDIO_PATH = \"/kaggle/input/filtereddata/audio/train\"\nTEST_AUDIO_PATH = \"/kaggle/input/filtereddata/audio/test\"\nVAL_AUDIO_PATH = \"/kaggle/input/filtereddata/audio/val\"\nTRAIN_CSV_PATH = \"/kaggle/input/filtereddata/audio/train_metadata.csv\"\nTEST_CSV_PATH = \"/kaggle/input/filtereddata/audio/test_metadata.csv\"\nVAL_CSV_PATH = \"/kaggle/input/filtereddata/audio/val_metadata.csv\"\n\n# Load metadata\ndef load_metadata():\n    \"\"\"Load metadata from CSV files\"\"\"\n    try:\n        train_df = pd.read_csv(TRAIN_CSV_PATH)\n        test_df = pd.read_csv(TEST_CSV_PATH)\n        val_df = pd.read_csv(VAL_CSV_PATH)\n\n        # Get the unique classes\n        all_classes = set(train_df[\"label\"].unique())\n        all_classes.update(test_df[\"label\"].unique())\n        all_classes.update(val_df[\"label\"].unique())\n\n        class_to_idx = {cls: i for i, cls in enumerate(sorted(all_classes))}\n        idx_to_class = {i: cls for cls, i in class_to_idx.items()}\n\n        print(f\"Total classes: {len(class_to_idx)}\")\n        return train_df, test_df, val_df, class_to_idx, idx_to_class\n\n    except Exception as e:\n        print(f\"Error loading metadata: {e}\")\n        raise\n\n\nclass AudioDataset(Dataset):\n    \"\"\"Dataset for audio classification with Mel spectrograms\"\"\"\n\n    def __init__(\n        self,\n        df,\n        audio_dir,\n        class_to_idx,\n        transform=None,\n        target_sample_rate=SAMPLE_RATE,\n    ):\n        \"\"\"\n        Args:\n            df: DataFrame with metadata\n            audio_dir: Directory with audio files\n            class_to_idx: Dictionary mapping class names to indices\n            transform: Optional transform to be applied on a sample\n            target_sample_rate: Sample rate to resample audio to\n        \"\"\"\n        self.df = df\n        self.audio_dir = audio_dir\n        self.class_to_idx = class_to_idx\n        self.transform = transform\n        self.target_sample_rate = target_sample_rate\n\n        # Ensure all files exist\n        self.valid_files = []\n        for i, row in df.iterrows():\n            file_path = os.path.join(audio_dir, row[\"fname\"])\n            if os.path.exists(file_path):\n                self.valid_files.append(i)\n\n        self.df = self.df.iloc[self.valid_files].reset_index(drop=True)\n        print(f\"Found {len(self.df)} valid audio files in {audio_dir}\")\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        # Get file path and label\n        row = self.df.iloc[idx]\n        file_path = os.path.join(self.audio_dir, row[\"fname\"])\n        label = self.class_to_idx[row[\"label\"]]\n\n        # Load and preprocess audio\n        try:\n            waveform, sample_rate = torchaudio.load(file_path)\n\n            # Convert to mono if needed\n            if waveform.shape[0] > 1:\n                waveform = torch.mean(waveform, dim=0, keepdim=True)\n\n            # Resample if needed\n            if sample_rate != self.target_sample_rate:\n                resampler = torchaudio.transforms.Resample(\n                    sample_rate, self.target_sample_rate\n                )\n                waveform = resampler(waveform)\n\n            # Pad or truncate to target length\n            if waveform.shape[1] < NUM_SAMPLES:\n                # Pad\n                padding = NUM_SAMPLES - waveform.shape[1]\n                waveform = F.pad(waveform, (0, padding))\n            elif waveform.shape[1] > NUM_SAMPLES:\n                # Truncate\n                waveform = waveform[:, :NUM_SAMPLES]\n\n            # Transform to mel spectrogram\n            mel_spectrogram = transforms.MelSpectrogram(\n                sample_rate=self.target_sample_rate,\n                n_fft=N_FFT,\n                hop_length=HOP_LENGTH,\n                n_mels=N_MELS,\n                f_min=FMIN,\n                f_max=FMAX,\n            )(waveform)\n\n            # Convert to decibels\n            mel_spectrogram = transforms.AmplitudeToDB()(mel_spectrogram)\n\n            # Apply transformations if provided\n            if self.transform:\n                mel_spectrogram = self.transform(mel_spectrogram)\n\n            return mel_spectrogram, label\n\n        except Exception as e:\n            print(f\"Error loading {file_path}: {e}\")\n            # Return a zero tensor and the label in case of error\n            return torch.zeros((1, N_MELS, NUM_SAMPLES // HOP_LENGTH + 1)), label\n\n\n# Data augmentation functions for audio\nclass WeakAugmentation:\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, mel_spec):\n        if random.random() < self.p:\n            # Time masking\n            time_mask_param = int(mel_spec.shape[2] * 0.1)  # 10% of time steps\n            time_mask = transforms.TimeMasking(time_mask_param)\n            mel_spec = time_mask(mel_spec)\n\n        return mel_spec\n\n\nclass StrongAugmentation:\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, mel_spec):\n        if random.random() < self.p:\n            # Time masking (more aggressive)\n            time_mask_param = int(mel_spec.shape[2] * 0.2)  # 20% of time steps\n            time_mask = transforms.TimeMasking(time_mask_param)\n            mel_spec = time_mask(mel_spec)\n\n            # Frequency masking\n            freq_mask_param = int(mel_spec.shape[1] * 0.2)  # 20% of frequency bins\n            freq_mask = transforms.FrequencyMasking(freq_mask_param)\n            mel_spec = freq_mask(mel_spec)\n\n            # Add random noise\n            noise = torch.randn_like(mel_spec) * 0.1\n            mel_spec = mel_spec + noise\n\n        return mel_spec\n\n\n# MobileNet model for audio classification\nclass AudioMobileNet(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super(AudioMobileNet, self).__init__()\n        # Load pre-trained MobileNet model\n        self.mobilenet = models.mobilenet_v2(weights=\"DEFAULT\")\n\n        # Modify the first convolutional layer to accept single-channel input\n        self.mobilenet.features[0][0] = nn.Conv2d(\n            1, 32, kernel_size=3, stride=2, padding=1, bias=False\n        )\n\n        # Replace the classifier\n        in_features = self.mobilenet.classifier[1].in_features\n        self.mobilenet.classifier = nn.Sequential(\n            nn.Dropout(0.2), nn.Linear(in_features, num_classes)\n        )\n\n    def forward(self, x):\n        return self.mobilenet(x)\n\n\n# Modified version for SeFOSS (features model f and classifier model g)\nclass SeFOSSMobileNet(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super(SeFOSSMobileNet, self).__init__()\n        # Feature extractor f\n        mobilenet = models.mobilenet_v2(weights=\"DEFAULT\")\n        mobilenet.features[0][0] = nn.Conv2d(\n            1, 32, kernel_size=3, stride=2, padding=1, bias=False\n        )\n        self.feature_extractor = nn.Sequential(\n            *list(mobilenet.features), nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten()\n        )\n\n        # Output dimension of feature extractor\n        self.feature_dim = 1280\n\n        # Classifier g\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2), nn.Linear(self.feature_dim, num_classes)\n        )\n\n    def forward(self, x):\n        features = self.feature_extractor(x)\n        logits = self.classifier(features)\n        return logits\n\n    def get_features(self, x):\n        return self.feature_extractor(x)\n\n    def classify(self, features):\n        return self.classifier(features)\n\n\n# SeFOSS implementation\nclass SeFOSS:\n    def __init__(self, num_classes=NUM_CLASSES, feature_dim=1280, device=device):\n        self.device = device\n        self.num_classes = num_classes\n        self.feature_dim = feature_dim\n\n        # Initialize models\n        self.model = SeFOSSMobileNet(num_classes=num_classes).to(device)\n\n        # Initialize optimizers\n        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            self.optimizer, \"min\", patience=5, factor=0.5\n        )\n\n        # Loss functions\n        self.criterion = nn.CrossEntropyLoss()\n\n        # Augmentations\n        self.weak_augmentation = WeakAugmentation(p=0.5)\n        self.strong_augmentation = StrongAugmentation(p=0.8)\n\n        # Thresholds\n        self.tau_d = 0.7  # Default value, will be tuned\n        self.tau_ood = 0.3  # Default value, will be tuned\n        self.tau_nood = 0.1  # Default value, will be tuned\n\n        # Loss weights\n        self.w_s = 1.0  # Supervised loss weight\n        self.w_p = 1.0  # Pseudo-label loss weight\n        self.w_e = 0.5  # Entropy loss weight\n        self.w_c = 0.5  # Consistency loss weight\n        self.w_u = 0.5  # Uncertainty loss weight\n\n        # History tracking\n        self.history = {\n            \"train_loss\": [],\n            \"val_loss\": [],\n            \"val_accuracy\": [],\n            \"train_accuracy\": [],\n        }\n\n    def pretrain(self, labeled_loader, val_loader, num_epochs=10):\n        \"\"\"Pretrain the model using only labeled data\"\"\"\n        print(\"Pretraining model...\")\n        best_val_loss = float(\"inf\")\n\n        for epoch in range(num_epochs):\n            # Training\n            self.model.train()\n            train_loss = 0.0\n            correct = 0\n            total = 0\n\n            for inputs, targets in labeled_loader:\n                inputs, targets = inputs.to(self.device), targets.to(self.device)\n\n                # Forward pass\n                self.optimizer.zero_grad()\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, targets)\n\n                # Backward pass\n                loss.backward()\n                self.optimizer.step()\n\n                # Statistics\n                train_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += targets.size(0)\n                correct += predicted.eq(targets).sum().item()\n\n            train_loss = train_loss / len(labeled_loader)\n            train_acc = 100.0 * correct / total\n\n            # Validation\n            val_loss, val_acc = self.evaluate(val_loader)\n\n            # Update scheduler\n            self.scheduler.step(val_loss)\n\n            # Save history\n            self.history[\"train_loss\"].append(train_loss)\n            self.history[\"val_loss\"].append(val_loss)\n            self.history[\"train_accuracy\"].append(train_acc)\n            self.history[\"val_accuracy\"].append(val_acc)\n\n            print(\n                f\"Pretrain Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\"\n            )\n\n            # Save best model\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                torch.save(\n                    self.model.state_dict(),\n                    os.path.join(output_dir, \"pretrained_model.pth\"),\n                )\n\n        # Load best model\n        self.model.load_state_dict(\n            torch.load(os.path.join(output_dir, \"pretrained_model.pth\"))\n        )\n        print(\"Pretraining completed!\")\n\n    def compute_confidence_thresholds(self, labeled_loader):\n        \"\"\"Compute confidence thresholds by evaluating on labeled data\"\"\"\n        print(\"Computing confidence thresholds...\")\n        self.model.eval()\n        all_confidences = []\n\n        with torch.no_grad():\n            for inputs, _ in labeled_loader:\n                inputs = inputs.to(self.device)\n                outputs = self.model(inputs)\n                probas = F.softmax(outputs, dim=1)\n                confidences, _ = torch.max(probas, dim=1)\n                all_confidences.extend(confidences.cpu().numpy())\n\n        # Convert to numpy array and sort\n        all_confidences = np.sort(all_confidences)\n        n = len(all_confidences)\n\n        # Set thresholds based on percentiles\n        self.tau_d = all_confidences[int(0.7 * n)]  # 70th percentile\n        self.tau_ood = all_confidences[int(0.3 * n)]  # 30th percentile\n        self.tau_nood = all_confidences[int(0.1 * n)]  # 10th percentile\n\n        print(\n            f\"Thresholds - tau_d: {self.tau_d:.4f}, tau_ood: {self.tau_ood:.4f}, tau_nood: {self.tau_nood:.4f}\"\n        )\n\n    def sefoss_train_step(self, labeled_batch, unlabeled_batch):\n        \"\"\"Perform a single SeFOSS training step\"\"\"\n        # Process labeled data\n        labeled_inputs, labeled_targets = labeled_batch\n        labeled_inputs, labeled_targets = labeled_inputs.to(\n            self.device\n        ), labeled_targets.to(self.device)\n\n        # Forward pass on labeled data\n        labeled_outputs = self.model(labeled_inputs)\n\n        # Supervised loss\n        l_s = self.criterion(labeled_outputs, labeled_targets)\n\n        # Process unlabeled data if provided\n        if unlabeled_batch is not None and len(unlabeled_batch[0]) > 0:\n            (\n                unlabeled_inputs,\n                _,\n            ) = unlabeled_batch  # Ignoring the labels for unlabeled data\n            unlabeled_inputs = unlabeled_inputs.to(self.device)\n\n            # Apply weak augmentation for each sample\n            weak_inputs = []\n            for i in range(unlabeled_inputs.size(0)):\n                weak_inputs.append(\n                    self.weak_augmentation(unlabeled_inputs[i].unsqueeze(0))\n                )\n            weak_inputs = torch.cat(weak_inputs, dim=0)\n\n            # Apply strong augmentation for each sample\n            strong_inputs = []\n            for i in range(unlabeled_inputs.size(0)):\n                strong_inputs.append(\n                    self.strong_augmentation(unlabeled_inputs[i].unsqueeze(0))\n                )\n            strong_inputs = torch.cat(strong_inputs, dim=0)\n\n            # Get predictions with weak augmentation\n            self.model.eval()\n            with torch.no_grad():\n                weak_outputs = self.model(weak_inputs)\n                weak_probs = F.softmax(weak_outputs, dim=1)\n                weak_confidences, pseudo_labels = torch.max(weak_probs, dim=1)\n\n            # Switch back to training mode\n            self.model.train()\n\n            # Get predictions with strong augmentation\n            strong_outputs = self.model(strong_inputs)\n            strong_probs = F.softmax(strong_outputs, dim=1)\n\n            # Compute entropy\n            entropy = -torch.sum(strong_probs * torch.log(strong_probs + 1e-10), dim=1)\n            mean_entropy = torch.mean(entropy)\n\n            # Create masks based on confidence thresholds\n            mask_d = weak_confidences >= self.tau_d\n            mask_ood = (weak_confidences < self.tau_d) & (\n                weak_confidences >= self.tau_ood\n            )\n            mask_nood = weak_confidences < self.tau_ood\n\n            # Pseudo-label loss\n            if torch.sum(mask_d) > 0:\n                l_p = F.cross_entropy(strong_outputs[mask_d], pseudo_labels[mask_d])\n            else:\n                l_p = torch.tensor(0.0).to(self.device)\n\n            # Entropy loss (minimizing entropy for uncertain samples)\n            if torch.sum(mask_ood) > 0:\n                l_e = torch.mean(entropy[mask_ood])\n            else:\n                l_e = torch.tensor(0.0).to(self.device)\n\n            # Consistency loss\n            if torch.sum(mask_d) > 0:\n                l_c = F.mse_loss(strong_probs[mask_d], weak_probs[mask_d])\n            else:\n                l_c = torch.tensor(0.0).to(self.device)\n\n            # Uncertainty loss\n            if torch.sum(mask_nood) > 0:\n                # Maximize entropy for noisy/OOD samples\n                l_u = -torch.mean(entropy[mask_nood])\n            else:\n                l_u = torch.tensor(0.0).to(self.device)\n\n        else:\n            # If no unlabeled data, set all losses to 0\n            l_p = torch.tensor(0.0).to(self.device)\n            l_e = torch.tensor(0.0).to(self.device)\n            l_c = torch.tensor(0.0).to(self.device)\n            l_u = torch.tensor(0.0).to(self.device)\n\n        # Compute total loss with adaptive weights\n        total_loss = (\n            self.w_s * l_s\n            + self.w_p * l_p\n            + self.w_e * l_e\n            + self.w_c * l_c\n            + self.w_u * l_u\n        )\n\n        return total_loss\n\n    def train(\n        self, labeled_loader, unlabeled_loader, val_loader, num_epochs=NUM_EPOCHS\n    ):\n        \"\"\"Train the model using SeFOSS algorithm\"\"\"\n        print(\"Starting SeFOSS training...\")\n        best_val_loss = float(\"inf\")\n\n        # First run pretraining\n        self.pretrain(labeled_loader, val_loader, num_epochs=5)\n\n        # Compute confidence thresholds\n        self.compute_confidence_thresholds(labeled_loader)\n\n        # Reset history\n        self.history = {\n            \"train_loss\": [],\n            \"val_loss\": [],\n            \"val_accuracy\": [],\n            \"train_accuracy\": [],\n        }\n\n        # Main training loop\n        for epoch in range(num_epochs):\n            self.model.train()\n            train_loss = 0.0\n\n            # Create iterator for unlabeled loader\n            if unlabeled_loader is not None:\n                unlabeled_iter = iter(unlabeled_loader)\n\n            # Train with labeled and unlabeled data\n            for batch_idx, labeled_batch in enumerate(labeled_loader):\n                # Get unlabeled batch if available\n                try:\n                    if unlabeled_loader is not None:\n                        unlabeled_batch = next(unlabeled_iter)\n                    else:\n                        unlabeled_batch = None\n                except StopIteration:\n                    if unlabeled_loader is not None:\n                        unlabeled_iter = iter(unlabeled_loader)\n                        try:\n                            unlabeled_batch = next(unlabeled_iter)\n                        except StopIteration:\n                            unlabeled_batch = None\n                    else:\n                        unlabeled_batch = None\n\n                # Compute SeFOSS loss\n                self.optimizer.zero_grad()\n                loss = self.sefoss_train_step(labeled_batch, unlabeled_batch)\n\n                # Backward pass\n                loss.backward()\n                self.optimizer.step()\n\n                train_loss += loss.item()\n\n            train_loss = train_loss / len(labeled_loader)\n\n            # Validation\n            val_loss, val_acc = self.evaluate(val_loader)\n\n            # Update scheduler\n            self.scheduler.step(val_loss)\n\n            # Compute train accuracy\n            train_acc = self.compute_accuracy(labeled_loader)\n\n            # Save history\n            self.history[\"train_loss\"].append(train_loss)\n            self.history[\"val_loss\"].append(val_loss)\n            self.history[\"train_accuracy\"].append(train_acc)\n            self.history[\"val_accuracy\"].append(val_acc)\n\n            print(\n                f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\"\n            )\n\n            # Save best model\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                torch.save(\n                    self.model.state_dict(), os.path.join(output_dir, \"best_model.pth\")\n                )\n\n        # Load best model\n        self.model.load_state_dict(\n            torch.load(os.path.join(output_dir, \"best_model.pth\"))\n        )\n\n        # Save trained model\n        torch.save(self.model, os.path.join(output_dir, \"full_model.pkl\"))\n\n        # Plot training history\n        self.plot_training_history()\n\n        print(\"Training completed!\")\n        return self.history\n\n    def evaluate(self, data_loader):\n        \"\"\"Evaluate the model on a given data loader\"\"\"\n        self.model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for inputs, targets in data_loader:\n                inputs, targets = inputs.to(self.device), targets.to(self.device)\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, targets)\n\n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += targets.size(0)\n                correct += predicted.eq(targets).sum().item()\n\n        val_loss = val_loss / len(data_loader)\n        val_acc = 100.0 * correct / total\n\n        return val_loss, val_acc\n\n    def compute_accuracy(self, data_loader):\n        \"\"\"Compute accuracy on a given data loader\"\"\"\n        self.model.eval()\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for inputs, targets in data_loader:\n                inputs, targets = inputs.to(self.device), targets.to(self.device)\n                outputs = self.model(inputs)\n                _, predicted = outputs.max(1)\n                total += targets.size(0)\n                correct += predicted.eq(targets).sum().item()\n\n        return 100.0 * correct / total\n\n    def plot_training_history(self):\n        \"\"\"Plot training and validation loss/accuracy\"\"\"\n        plt.figure(figsize=(12, 5))\n\n        # Plot loss\n        plt.subplot(1, 2, 1)\n        plt.plot(self.history[\"train_loss\"], label=\"Train Loss\")\n        plt.plot(self.history[\"val_loss\"], label=\"Validation Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"Training and Validation Loss\")\n        plt.legend()\n\n        # Plot accuracy\n        plt.subplot(1, 2, 2)\n        plt.plot(self.history[\"train_accuracy\"], label=\"Train Accuracy\")\n        plt.plot(self.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Accuracy (%)\")\n        plt.title(\"Training and Validation Accuracy\")\n        plt.legend()\n\n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, \"training_history.png\"))\n        plt.close()\n\n    def predict(self, data_loader):\n        \"\"\"Make predictions on a given data loader\"\"\"\n        self.model.eval()\n        all_preds = []\n        all_targets = []\n\n        with torch.no_grad():\n            for inputs, targets in data_loader:\n                inputs = inputs.to(self.device)\n                outputs = self.model(inputs)\n                _, preds = outputs.max(1)\n\n                all_preds.extend(preds.cpu().numpy())\n                all_targets.extend(targets.numpy())\n\n        return np.array(all_preds), np.array(all_targets)\n\n    def evaluate_metrics(self, data_loader, idx_to_class=None):\n        \"\"\"Evaluate the model using standard classification metrics\"\"\"\n        preds, targets = self.predict(data_loader)\n\n        # Calculate metrics\n        acc = accuracy_score(targets, preds)\n        prec = precision_score(targets, preds, average=\"weighted\", zero_division=0)\n        rec = recall_score(targets, preds, average=\"weighted\", zero_division=0)\n        f1 = f1_score(targets, preds, average=\"weighted\", zero_division=0)\n\n        # Create confusion matrix\n        cm = confusion_matrix(targets, preds)\n\n        # Print metrics\n        print(f\"Accuracy: {acc:.4f}\")\n        print(f\"Precision: {prec:.4f}\")\n        print(f\"Recall: {rec:.4f}\")\n        print(f\"F1 Score: {f1:.4f}\")\n\n        # Plot confusion matrix if number of classes is reasonable\n        if len(np.unique(targets)) <= 20:\n            plt.figure(figsize=(10, 8))\n            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n            plt.xlabel(\"Predicted\")\n            plt.ylabel(\"True\")\n            plt.title(\"Confusion Matrix\")\n            plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n            plt.close()\n\n        return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n\n\ndef create_balanced_data_splits(\n    train_dataset, val_dataset, test_dataset, labeled_samples_per_class, min_samples=2\n):\n    \"\"\"Create labeled and unlabeled datasets for semi-supervised learning with balanced class distribution\"\"\"\n    # Get all training samples by class\n    train_samples_by_class = defaultdict(list)\n    for idx in range(len(train_dataset)):\n        _, label = train_dataset[idx]\n        \n        # Fix: Handle both tensor and int types for label\n        if hasattr(label, 'item'):\n            label = label.item()  # Convert tensor to int if it's a tensor\n            \n        train_samples_by_class[label].append(idx)\n\n    # Select labeled samples\n    labeled_indices = []\n    unlabeled_indices = []\n\n    for label, indices in train_samples_by_class.items():\n        # Ensure we have enough samples for this class\n        num_available = len(indices)\n        num_to_use = min(labeled_samples_per_class, num_available)\n\n        if num_to_use > 0:\n            # Select random samples for labeled set\n            selected_indices = random.sample(indices, num_to_use)\n            labeled_indices.extend(selected_indices)\n\n            # Remaining samples go to unlabeled set\n            remaining_indices = [idx for idx in indices if idx not in selected_indices]\n            unlabeled_indices.extend(remaining_indices)\n        else:\n            print(f\"Warning: Class {label} has no samples\")\n\n    print(\n        f\"Labeled samples: {len(labeled_indices)}, Unlabeled samples: {len(unlabeled_indices)}\"\n    )\n\n    # Create subset datasets\n    labeled_dataset = Subset(train_dataset, labeled_indices)\n    unlabeled_dataset = (\n        Subset(train_dataset, unlabeled_indices) if len(unlabeled_indices) > 0 else None\n    )\n\n    return labeled_dataset, unlabeled_dataset, val_dataset, test_dataset\n\ndef run_sefoss_training(labeled_samples_per_class):\n    \"\"\"Run SeFOSS training with a specific number of labeled samples per class\"\"\"\n    print(f\"{'='*50}\")\n    print(f\"Running SeFOSS with {labeled_samples_per_class} labeled samples per class\")\n    print(f\"{'='*50}\")\n\n    # Load metadata\n    train_df, test_df, val_df, class_to_idx, idx_to_class = load_metadata()\n\n    # Create datasets\n    train_dataset = AudioDataset(train_df, TRAIN_AUDIO_PATH, class_to_idx)\n    test_dataset = AudioDataset(test_df, TEST_AUDIO_PATH, class_to_idx)\n    val_dataset = AudioDataset(val_df, VAL_AUDIO_PATH, class_to_idx)\n\n    # Create balanced data splits\n    (\n        labeled_dataset,\n        unlabeled_dataset,\n        val_dataset,\n        test_dataset,\n    ) = create_balanced_data_splits(\n        train_dataset, val_dataset, test_dataset, labeled_samples_per_class\n    )\n\n    # Create data loaders\n    labeled_loader = DataLoader(\n        labeled_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True,\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True,\n    )\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True,\n    )\n\n    # Create unlabeled loader if we have unlabeled data\n    if unlabeled_dataset is not None and len(unlabeled_dataset) > 0:\n        unlabeled_loader = DataLoader(\n            unlabeled_dataset,\n            batch_size=BATCH_SIZE,\n            shuffle=True,\n            num_workers=2,\n            pin_memory=True,\n        )\n    else:\n        unlabeled_loader = None\n        print(\"Warning: No unlabeled data available\")\n\n    # Initialize SeFOSS\n    sefoss = SeFOSS(num_classes=len(class_to_idx), feature_dim=1280, device=device)\n\n    # Train model\n    start_time = time.time()\n    sefoss.train(labeled_loader, unlabeled_loader, val_loader, num_epochs=NUM_EPOCHS)\n    training_time = time.time() - start_time\n    print(f\"Training completed in {training_time:.2f} seconds\")\n\n    # Evaluate on test set\n    print(\"Evaluating on test set...\")\n    metrics = sefoss.evaluate_metrics(test_loader, idx_to_class)\n\n    # Save metrics to CSV\n    metrics_df = pd.DataFrame([metrics])\n    metrics_df[\"labeled_samples_per_class\"] = labeled_samples_per_class\n    metrics_df[\"training_time\"] = training_time\n    metrics_df.to_csv(\n        os.path.join(output_dir, f\"metrics_{labeled_samples_per_class}.csv\"),\n        index=False,\n    )\n\n    # Return metrics\n    return sefoss, metrics\n\n\ndef plot_comparison_results(results):\n    \"\"\"Plot comparison of results for different numbers of labeled samples\"\"\"\n    samples = list(results.keys())\n    accuracies = [results[s][\"accuracy\"] for s in samples]\n    precisions = [results[s][\"precision\"] for s in samples]\n    recalls = [results[s][\"recall\"] for s in samples]\n    f1_scores = [results[s][\"f1\"] for s in samples]\n\n    # Create dataframe for results\n    df = pd.DataFrame(\n        {\n            \"Labeled Samples Per Class\": samples,\n            \"Accuracy\": accuracies,\n            \"Precision\": precisions,\n            \"Recall\": recalls,\n            \"F1 Score\": f1_scores,\n        }\n    )\n\n    # Save to CSV\n    df.to_csv(os.path.join(output_dir, \"comparison_results.csv\"), index=False)\n\n    # Plot results\n    plt.figure(figsize=(12, 8))\n\n    plt.subplot(2, 2, 1)\n    plt.plot(samples, accuracies, \"o-\", linewidth=2)\n    plt.xlabel(\"Labeled Samples Per Class\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Accuracy vs Labeled Samples\")\n    plt.grid(True)\n\n    plt.subplot(2, 2, 2)\n    plt.plot(samples, precisions, \"o-\", linewidth=2)\n    plt.xlabel(\"Labeled Samples Per Class\")\n    plt.ylabel(\"Precision\")\n    plt.title(\"Precision vs Labeled Samples\")\n    plt.grid(True)\n\n    plt.subplot(2, 2, 3)\n    plt.plot(samples, recalls, \"o-\", linewidth=2)\n    plt.xlabel(\"Labeled Samples Per Class\")\n    plt.ylabel(\"Recall\")\n    plt.title(\"Recall vs Labeled Samples\")\n    plt.grid(True)\n\n    plt.subplot(2, 2, 4)\n    plt.plot(samples, f1_scores, \"o-\", linewidth=2)\n    plt.xlabel(\"Labeled Samples Per Class\")\n    plt.ylabel(\"F1 Score\")\n    plt.title(\"F1 Score vs Labeled Samples\")\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.savefig(os.path.join(output_dir, \"comparison_results.png\"))\n    plt.close()\n\n    return df\n\n\ndef analyze_model_performance(model, test_loader, idx_to_class):\n    \"\"\"Analyze model performance in detail\"\"\"\n    model.eval()\n    all_preds = []\n    all_targets = []\n    all_confidences = []\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            outputs = model.model(inputs)\n            probs = F.softmax(outputs, dim=1)\n            confidences, preds = torch.max(probs, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(targets.numpy())\n            all_confidences.extend(confidences.cpu().numpy())\n\n    # Convert to numpy arrays\n    all_preds = np.array(all_preds)\n    all_targets = np.array(all_targets)\n    all_confidences = np.array(all_confidences)\n\n    # Calculate per-class metrics\n    classes = sorted(idx_to_class.keys())\n    class_names = [idx_to_class[i] for i in classes]\n\n    per_class_metrics = []\n    for cls in classes:\n        cls_indices = all_targets == cls\n        if np.sum(cls_indices) > 0:\n            cls_acc = np.mean(all_preds[cls_indices] == cls)\n            cls_count = np.sum(cls_indices)\n            cls_conf = np.mean(all_confidences[cls_indices])\n            per_class_metrics.append(\n                {\n                    \"class_id\": cls,\n                    \"class_name\": idx_to_class[cls],\n                    \"accuracy\": cls_acc,\n                    \"count\": cls_count,\n                    \"confidence\": cls_conf,\n                }\n            )\n\n    # Create dataframe and save\n    per_class_df = pd.DataFrame(per_class_metrics)\n    per_class_df.to_csv(os.path.join(output_dir, \"per_class_metrics.csv\"), index=False)\n\n    # Plot per-class accuracy\n    plt.figure(figsize=(12, 6))\n    plt.bar(per_class_df[\"class_name\"], per_class_df[\"accuracy\"])\n    plt.xlabel(\"Class\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Per-Class Accuracy\")\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.savefig(os.path.join(output_dir, \"per_class_accuracy.png\"))\n    plt.close()\n\n    # Plot confidence distribution\n    plt.figure(figsize=(10, 6))\n    plt.hist(all_confidences, bins=20)\n    plt.xlabel(\"Confidence\")\n    plt.ylabel(\"Count\")\n    plt.title(\"Model Confidence Distribution\")\n    plt.grid(True)\n    plt.savefig(os.path.join(output_dir, \"confidence_distribution.png\"))\n    plt.close()\n\n    # Analyze mistakes - find examples with high confidence but wrong prediction\n    high_conf_mistakes = []\n    for i in range(len(all_preds)):\n        if all_preds[i] != all_targets[i] and all_confidences[i] > 0.8:\n            high_conf_mistakes.append(\n                {\n                    \"true\": idx_to_class[all_targets[i]],\n                    \"pred\": idx_to_class[all_preds[i]],\n                    \"confidence\": all_confidences[i],\n                }\n            )\n\n    if high_conf_mistakes:\n        mistakes_df = pd.DataFrame(high_conf_mistakes)\n        mistakes_df.to_csv(\n            os.path.join(output_dir, \"high_confidence_mistakes.csv\"), index=False\n        )\n\n    return per_class_df\n\n\ndef fix_sefoss_train_step(self, labeled_batch, unlabeled_batch):\n    \"\"\"A fixed version of sefoss_train_step to handle list inputs properly\"\"\"\n    # Process labeled data\n    labeled_inputs, labeled_targets = labeled_batch\n    labeled_inputs, labeled_targets = labeled_inputs.to(\n        self.device\n    ), labeled_targets.to(self.device)\n\n    # Forward pass on labeled data\n    labeled_outputs = self.model(labeled_inputs)\n\n    # Supervised loss\n    l_s = self.criterion(labeled_outputs, labeled_targets)\n\n    # Process unlabeled data if provided\n    if (\n        unlabeled_batch is not None\n        and isinstance(unlabeled_batch, list)\n        and len(unlabeled_batch) > 0\n    ):\n        unlabeled_inputs, _ = unlabeled_batch  # Ignoring the labels for unlabeled data\n\n        # Handle the case when unlabeled_inputs is a list\n        if isinstance(unlabeled_inputs, list):\n            if len(unlabeled_inputs) == 0:\n                # No unlabeled data available\n                l_p = torch.tensor(0.0).to(self.device)\n                l_e = torch.tensor(0.0).to(self.device)\n                l_c = torch.tensor(0.0).to(self.device)\n                l_u = torch.tensor(0.0).to(self.device)\n            else:\n                unlabeled_inputs = torch.stack(unlabeled_inputs).to(self.device)\n                # Continue with regular processing\n        else:\n            unlabeled_inputs = unlabeled_inputs.to(self.device)\n\n        # Only proceed if we have unlabeled data\n        if isinstance(unlabeled_inputs, torch.Tensor) and unlabeled_inputs.size(0) > 0:\n            # Apply weak augmentation for each sample\n            weak_inputs = []\n            for i in range(unlabeled_inputs.size(0)):\n                weak_inputs.append(\n                    self.weak_augmentation(unlabeled_inputs[i].unsqueeze(0))\n                )\n            weak_inputs = torch.cat(weak_inputs, dim=0)\n\n            # Apply strong augmentation for each sample\n            strong_inputs = []\n            for i in range(unlabeled_inputs.size(0)):\n                strong_inputs.append(\n                    self.strong_augmentation(unlabeled_inputs[i].unsqueeze(0))\n                )\n            strong_inputs = torch.cat(strong_inputs, dim=0)\n\n            # Get predictions with weak augmentation\n            self.model.eval()\n            with torch.no_grad():\n                weak_outputs = self.model(weak_inputs)\n                weak_probs = F.softmax(weak_outputs, dim=1)\n                weak_confidences, pseudo_labels = torch.max(weak_probs, dim=1)\n\n            # Switch back to training mode\n            self.model.train()\n\n            # Get predictions with strong augmentation\n            strong_outputs = self.model(strong_inputs)\n            strong_probs = F.softmax(strong_outputs, dim=1)\n\n            # Compute entropy\n            entropy = -torch.sum(strong_probs * torch.log(strong_probs + 1e-10), dim=1)\n            mean_entropy = torch.mean(entropy)\n\n            # Create masks based on confidence thresholds\n            mask_d = weak_confidences >= self.tau_d\n            mask_ood = (weak_confidences < self.tau_d) & (\n                weak_confidences >= self.tau_ood\n            )\n            mask_nood = weak_confidences < self.tau_ood\n\n            # Pseudo-label loss\n            if torch.sum(mask_d) > 0:\n                l_p = F.cross_entropy(strong_outputs[mask_d], pseudo_labels[mask_d])\n            else:\n                l_p = torch.tensor(0.0).to(self.device)\n\n            # Entropy loss (minimizing entropy for uncertain samples)\n            if torch.sum(mask_ood) > 0:\n                l_e = torch.mean(entropy[mask_ood])\n            else:\n                l_e = torch.tensor(0.0).to(self.device)\n\n            # Consistency loss\n            if torch.sum(mask_d) > 0:\n                l_c = F.mse_loss(strong_probs[mask_d], weak_probs[mask_d])\n            else:\n                l_c = torch.tensor(0.0).to(self.device)\n\n            # Uncertainty loss\n            if torch.sum(mask_nood) > 0:\n                # Maximize entropy for noisy/OOD samples\n                l_u = -torch.mean(entropy[mask_nood])\n            else:\n                l_u = torch.tensor(0.0).to(self.device)\n        else:\n            # No valid unlabeled data tensor\n            l_p = torch.tensor(0.0).to(self.device)\n            l_e = torch.tensor(0.0).to(self.device)\n            l_c = torch.tensor(0.0).to(self.device)\n            l_u = torch.tensor(0.0).to(self.device)\n    else:\n        # No unlabeled batch provided\n        l_p = torch.tensor(0.0).to(self.device)\n        l_e = torch.tensor(0.0).to(self.device)\n        l_c = torch.tensor(0.0).to(self.device)\n        l_u = torch.tensor(0.0).to(self.device)\n\n    # Compute total loss with adaptive weights\n    total_loss = (\n        self.w_s * l_s\n        + self.w_p * l_p\n        + self.w_e * l_e\n        + self.w_c * l_c\n        + self.w_u * l_u\n    )\n\n    return total_loss\n\n\n# Fix the SeFOSS class by monkey-patching the problematic method\nSeFOSS.sefoss_train_step = fix_sefoss_train_step\n\n\ndef main_fixed():\n    \"\"\"Main function to run all experiments with fixed error handling\"\"\"\n    print(\"Starting SeFOSS experiments on FSDKaggle2018 dataset...\")\n\n    # Create output directory\n    global output_dir\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Run experiments with different numbers of labeled samples\n    labeled_samples_sizes = [40, 100, 200, 400]\n    results = {}\n\n    for size in labeled_samples_sizes:\n        print(f\"{'='*50}\")\n        print(f\"Running experiment with {size} labeled samples per class\")\n        print(f\"{'='*50}\")\n\n        try:\n            # Create directory for this experiment\n            exp_dir = os.path.join(output_dir, f\"samples_{size}\")\n            os.makedirs(exp_dir, exist_ok=True)\n\n            # Temporarily change output directory\n            \n            original_output_dir = output_dir\n            output_dir = exp_dir\n\n            # Run SeFOSS training\n            sefoss, metrics = run_sefoss_training(labeled_samples_per_class=size)\n            results[size] = metrics\n\n            # Load metadata for analysis\n            _, _, _, class_to_idx, idx_to_class = load_metadata()\n\n            # Create test loader for analysis\n            test_df = pd.read_csv(TEST_CSV_PATH)\n            test_dataset = AudioDataset(test_df, TEST_AUDIO_PATH, class_to_idx)\n            test_loader = DataLoader(\n                test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n            )\n\n            # Analyze model performance\n            analyze_model_performance(sefoss, test_loader, idx_to_class)\n\n            # Reset output directory\n            output_dir = original_output_dir\n\n        except Exception as e:\n            print(f\"Error in experiment with {size} labeled samples per class: {e}\")\n            import traceback\n\n            traceback.print_exc()\n            # Reset output directory\n            output_dir = original_output_dir\n            # Store None for this experiment\n            results[size] = {\n                \"accuracy\": 0.0,\n                \"precision\": 0.0,\n                \"recall\": 0.0,\n                \"f1\": 0.0,\n            }\n\n    # Plot comparison of results\n    comparison_df = plot_comparison_results(results)\n    print(\"Experiment completed successfully!\")\n    print(comparison_df)\n\n    return results\n\n\nif __name__ == \"__main__\":\n    main_fixed()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T18:55:39.255474Z","iopub.execute_input":"2025-04-14T18:55:39.256101Z","iopub.status.idle":"2025-04-14T20:48:21.334537Z","shell.execute_reply.started":"2025-04-14T18:55:39.256077Z","shell.execute_reply":"2025-04-14T20:48:21.333610Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nStarting SeFOSS experiments on FSDKaggle2018 dataset...\n==================================================\nRunning experiment with 40 labeled samples per class\n==================================================\n==================================================\nRunning SeFOSS with 40 labeled samples per class\n==================================================\nTotal classes: 41\nFound 1000 valid audio files in /kaggle/input/filtereddata/audio/train\nFound 1000 valid audio files in /kaggle/input/filtereddata/audio/test\nFound 1000 valid audio files in /kaggle/input/filtereddata/audio/val\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n","output_type":"stream"},{"name":"stdout","text":"Labeled samples: 996, Unlabeled samples: 4\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13.6M/13.6M [00:00<00:00, 122MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Starting SeFOSS training...\nPretraining model...\nPretrain Epoch 1/5 | Train Loss: 3.3218 | Train Acc: 13.86% | Val Loss: 2.8051 | Val Acc: 24.40%\nPretrain Epoch 2/5 | Train Loss: 2.2686 | Train Acc: 36.65% | Val Loss: 2.2851 | Val Acc: 32.80%\nPretrain Epoch 3/5 | Train Loss: 1.6716 | Train Acc: 51.51% | Val Loss: 1.9732 | Val Acc: 47.60%\nPretrain Epoch 4/5 | Train Loss: 1.1831 | Train Acc: 67.37% | Val Loss: 1.7918 | Val Acc: 52.40%\nPretrain Epoch 5/5 | Train Loss: 0.8190 | Train Acc: 77.71% | Val Loss: 1.7824 | Val Acc: 53.30%\nPretraining completed!\nComputing confidence thresholds...\nThresholds - tau_d: 0.9614, tau_ood: 0.7395, tau_nood: 0.4651\nEpoch 1/50 | Train Loss: -0.7875 | Train Acc: 74.70% | Val Loss: 2.1447 | Val Acc: 45.70%\nEpoch 2/50 | Train Loss: -1.1925 | Train Acc: 84.34% | Val Loss: 1.9823 | Val Acc: 46.30%\nEpoch 3/50 | Train Loss: -1.3707 | Train Acc: 90.26% | Val Loss: 1.8705 | Val Acc: 52.70%\nEpoch 4/50 | Train Loss: -1.5291 | Train Acc: 86.45% | Val Loss: 2.1428 | Val Acc: 47.30%\nEpoch 5/50 | Train Loss: -1.5903 | Train Acc: 88.86% | Val Loss: 1.9362 | Val Acc: 51.50%\nEpoch 6/50 | Train Loss: -1.6385 | Train Acc: 91.57% | Val Loss: 1.9911 | Val Acc: 51.10%\nEpoch 7/50 | Train Loss: -1.7271 | Train Acc: 96.29% | Val Loss: 1.8759 | Val Acc: 54.00%\nEpoch 8/50 | Train Loss: -1.7963 | Train Acc: 98.09% | Val Loss: 1.8267 | Val Acc: 57.50%\nEpoch 9/50 | Train Loss: -1.8134 | Train Acc: 96.59% | Val Loss: 1.9052 | Val Acc: 56.10%\nEpoch 10/50 | Train Loss: -1.8127 | Train Acc: 98.29% | Val Loss: 1.9692 | Val Acc: 56.40%\nEpoch 11/50 | Train Loss: -1.7957 | Train Acc: 98.59% | Val Loss: 1.8081 | Val Acc: 58.70%\nEpoch 12/50 | Train Loss: -1.7789 | Train Acc: 98.80% | Val Loss: 1.8176 | Val Acc: 56.40%\nEpoch 13/50 | Train Loss: -1.7811 | Train Acc: 99.30% | Val Loss: 1.7283 | Val Acc: 59.90%\nEpoch 14/50 | Train Loss: -1.8151 | Train Acc: 97.19% | Val Loss: 1.9254 | Val Acc: 56.20%\nEpoch 15/50 | Train Loss: -1.7867 | Train Acc: 98.80% | Val Loss: 1.8530 | Val Acc: 57.40%\nEpoch 16/50 | Train Loss: -1.8324 | Train Acc: 99.10% | Val Loss: 1.9154 | Val Acc: 57.10%\nEpoch 17/50 | Train Loss: -1.8313 | Train Acc: 99.80% | Val Loss: 1.8196 | Val Acc: 59.20%\nEpoch 18/50 | Train Loss: -1.8208 | Train Acc: 99.30% | Val Loss: 1.9682 | Val Acc: 56.00%\nEpoch 19/50 | Train Loss: -1.8376 | Train Acc: 99.80% | Val Loss: 1.9135 | Val Acc: 59.90%\nEpoch 20/50 | Train Loss: -1.7719 | Train Acc: 99.60% | Val Loss: 1.8946 | Val Acc: 60.10%\nEpoch 21/50 | Train Loss: -1.8431 | Train Acc: 99.90% | Val Loss: 1.8153 | Val Acc: 60.80%\nEpoch 22/50 | Train Loss: -1.8299 | Train Acc: 99.80% | Val Loss: 1.9064 | Val Acc: 58.10%\nEpoch 23/50 | Train Loss: -1.8394 | Train Acc: 99.40% | Val Loss: 1.9636 | Val Acc: 58.90%\nEpoch 24/50 | Train Loss: -1.8259 | Train Acc: 99.60% | Val Loss: 1.9048 | Val Acc: 58.30%\nEpoch 25/50 | Train Loss: -1.8383 | Train Acc: 99.70% | Val Loss: 1.9072 | Val Acc: 58.50%\nEpoch 26/50 | Train Loss: -1.8243 | Train Acc: 99.80% | Val Loss: 1.8706 | Val Acc: 58.80%\nEpoch 27/50 | Train Loss: -1.8133 | Train Acc: 99.20% | Val Loss: 1.9326 | Val Acc: 59.00%\nEpoch 28/50 | Train Loss: -1.8263 | Train Acc: 100.00% | Val Loss: 1.8282 | Val Acc: 61.30%\nEpoch 29/50 | Train Loss: -1.8317 | Train Acc: 99.20% | Val Loss: 1.9495 | Val Acc: 58.10%\nEpoch 30/50 | Train Loss: -1.8150 | Train Acc: 99.70% | Val Loss: 1.8831 | Val Acc: 59.20%\nEpoch 31/50 | Train Loss: -1.8427 | Train Acc: 99.70% | Val Loss: 1.8960 | Val Acc: 58.40%\nEpoch 32/50 | Train Loss: -1.8138 | Train Acc: 99.70% | Val Loss: 1.9205 | Val Acc: 58.70%\nEpoch 33/50 | Train Loss: -1.8100 | Train Acc: 99.70% | Val Loss: 1.7897 | Val Acc: 59.40%\nEpoch 34/50 | Train Loss: -1.8389 | Train Acc: 99.70% | Val Loss: 1.8857 | Val Acc: 58.50%\nEpoch 35/50 | Train Loss: -1.8206 | Train Acc: 99.70% | Val Loss: 1.8140 | Val Acc: 60.80%\nEpoch 36/50 | Train Loss: -1.8381 | Train Acc: 99.70% | Val Loss: 1.8744 | Val Acc: 59.60%\nEpoch 37/50 | Train Loss: -1.8403 | Train Acc: 99.60% | Val Loss: 1.9156 | Val Acc: 58.60%\nEpoch 38/50 | Train Loss: -1.8283 | Train Acc: 99.50% | Val Loss: 1.9524 | Val Acc: 58.00%\nEpoch 39/50 | Train Loss: -1.8082 | Train Acc: 99.80% | Val Loss: 1.8298 | Val Acc: 60.70%\nEpoch 40/50 | Train Loss: -1.8248 | Train Acc: 99.60% | Val Loss: 1.9388 | Val Acc: 57.90%\nEpoch 41/50 | Train Loss: -1.8402 | Train Acc: 99.40% | Val Loss: 1.9972 | Val Acc: 57.80%\nEpoch 42/50 | Train Loss: -1.8169 | Train Acc: 99.70% | Val Loss: 1.9165 | Val Acc: 58.50%\nEpoch 43/50 | Train Loss: -1.8262 | Train Acc: 99.70% | Val Loss: 1.9179 | Val Acc: 59.00%\nEpoch 44/50 | Train Loss: -1.8416 | Train Acc: 99.70% | Val Loss: 1.8510 | Val Acc: 59.50%\nEpoch 45/50 | Train Loss: -1.8481 | Train Acc: 99.70% | Val Loss: 1.9285 | Val Acc: 57.50%\nEpoch 46/50 | Train Loss: -1.8473 | Train Acc: 99.70% | Val Loss: 1.8552 | Val Acc: 58.40%\nEpoch 47/50 | Train Loss: -1.8482 | Train Acc: 99.60% | Val Loss: 1.9608 | Val Acc: 57.70%\nEpoch 48/50 | Train Loss: -1.8216 | Train Acc: 99.50% | Val Loss: 1.9330 | Val Acc: 58.30%\nEpoch 49/50 | Train Loss: -1.8314 | Train Acc: 99.90% | Val Loss: 1.9355 | Val Acc: 58.30%\nEpoch 50/50 | Train Loss: -1.8469 | Train Acc: 99.70% | Val Loss: 1.9481 | Val Acc: 57.90%\nTraining completed!\nTraining completed in 1788.30 seconds\nEvaluating on test set...\nAccuracy: 0.6250\nPrecision: 0.6575\nRecall: 0.6250\nF1 Score: 0.6211\nTotal classes: 41\nFound 1000 valid audio files in /kaggle/input/filtereddata/audio/test\nError in experiment with 40 labeled samples per class: 'SeFOSS' object has no attribute 'eval'\n==================================================\nRunning experiment with 100 labeled samples per class\n==================================================\n==================================================\nRunning SeFOSS with 100 labeled samples per class\n==================================================\nTotal classes: 41\nFound 1000 valid audio files in /kaggle/input/filtereddata/audio/train\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/444710727.py\", line 1196, in main_fixed\n    analyze_model_performance(sefoss, test_loader, idx_to_class)\n  File \"/tmp/ipykernel_31/444710727.py\", line 934, in analyze_model_performance\n    model.eval()\n    ^^^^^^^^^^\nAttributeError: 'SeFOSS' object has no attribute 'eval'\n","output_type":"stream"},{"name":"stdout","text":"Found 1000 valid audio files in /kaggle/input/filtereddata/audio/test\nFound 1000 valid audio files in /kaggle/input/filtereddata/audio/val\nLabeled samples: 1000, Unlabeled samples: 0\nWarning: No unlabeled data available\nStarting SeFOSS training...\nPretraining model...\nPretrain Epoch 1/5 | Train Loss: 3.3533 | Train Acc: 13.80% | Val Loss: 2.8655 | Val Acc: 22.00%\nPretrain Epoch 2/5 | Train Loss: 2.3792 | Train Acc: 34.40% | Val Loss: 2.5738 | Val Acc: 27.50%\nPretrain Epoch 3/5 | Train Loss: 1.6552 | Train Acc: 52.20% | Val Loss: 2.0045 | Val Acc: 43.00%\nPretrain Epoch 4/5 | Train Loss: 1.1209 | Train Acc: 68.70% | Val Loss: 1.7053 | Val Acc: 53.50%\nPretrain Epoch 5/5 | Train Loss: 0.8409 | Train Acc: 76.70% | Val Loss: 1.7742 | Val Acc: 52.00%\nPretraining completed!\nComputing confidence thresholds...\nThresholds - tau_d: 0.8964, tau_ood: 0.5137, tau_nood: 0.2977\nEpoch 1/50 | Train Loss: 0.8144 | Train Acc: 91.50% | Val Loss: 1.7109 | Val Acc: 54.40%\nEpoch 2/50 | Train Loss: 0.5404 | Train Acc: 95.70% | Val Loss: 1.6488 | Val Acc: 56.90%\nEpoch 3/50 | Train Loss: 0.3400 | Train Acc: 97.90% | Val Loss: 1.5097 | Val Acc: 61.00%\nEpoch 4/50 | Train Loss: 0.2807 | Train Acc: 97.20% | Val Loss: 1.6398 | Val Acc: 61.80%\nEpoch 5/50 | Train Loss: 0.3273 | Train Acc: 95.40% | Val Loss: 1.8472 | Val Acc: 58.20%\nEpoch 6/50 | Train Loss: 0.2464 | Train Acc: 97.50% | Val Loss: 2.0003 | Val Acc: 52.90%\nEpoch 7/50 | Train Loss: 0.1734 | Train Acc: 98.90% | Val Loss: 1.6690 | Val Acc: 59.70%\nEpoch 8/50 | Train Loss: 0.1641 | Train Acc: 99.60% | Val Loss: 1.7647 | Val Acc: 59.10%\nEpoch 9/50 | Train Loss: 0.1812 | Train Acc: 98.90% | Val Loss: 1.7841 | Val Acc: 60.60%\nEpoch 10/50 | Train Loss: 0.0939 | Train Acc: 99.90% | Val Loss: 1.5511 | Val Acc: 62.80%\nEpoch 11/50 | Train Loss: 0.0574 | Train Acc: 100.00% | Val Loss: 1.5159 | Val Acc: 64.80%\nEpoch 12/50 | Train Loss: 0.0507 | Train Acc: 100.00% | Val Loss: 1.5932 | Val Acc: 63.40%\nEpoch 13/50 | Train Loss: 0.0410 | Train Acc: 100.00% | Val Loss: 1.6394 | Val Acc: 63.30%\nEpoch 14/50 | Train Loss: 0.0290 | Train Acc: 100.00% | Val Loss: 1.5924 | Val Acc: 64.10%\nEpoch 15/50 | Train Loss: 0.0243 | Train Acc: 100.00% | Val Loss: 1.5839 | Val Acc: 64.90%\nEpoch 16/50 | Train Loss: 0.0148 | Train Acc: 100.00% | Val Loss: 1.5478 | Val Acc: 65.30%\nEpoch 17/50 | Train Loss: 0.0106 | Train Acc: 100.00% | Val Loss: 1.5351 | Val Acc: 65.20%\nEpoch 18/50 | Train Loss: 0.0199 | Train Acc: 100.00% | Val Loss: 1.5731 | Val Acc: 64.60%\nEpoch 19/50 | Train Loss: 0.0147 | Train Acc: 100.00% | Val Loss: 1.5845 | Val Acc: 65.10%\nEpoch 20/50 | Train Loss: 0.0098 | Train Acc: 100.00% | Val Loss: 1.5447 | Val Acc: 64.60%\nEpoch 21/50 | Train Loss: 0.0057 | Train Acc: 100.00% | Val Loss: 1.5534 | Val Acc: 65.00%\nEpoch 22/50 | Train Loss: 0.0064 | Train Acc: 100.00% | Val Loss: 1.5467 | Val Acc: 65.30%\nEpoch 23/50 | Train Loss: 0.0142 | Train Acc: 100.00% | Val Loss: 1.5785 | Val Acc: 65.50%\nEpoch 24/50 | Train Loss: 0.0122 | Train Acc: 100.00% | Val Loss: 1.5616 | Val Acc: 64.60%\nEpoch 25/50 | Train Loss: 0.0056 | Train Acc: 100.00% | Val Loss: 1.5484 | Val Acc: 65.30%\nEpoch 26/50 | Train Loss: 0.0074 | Train Acc: 100.00% | Val Loss: 1.5458 | Val Acc: 66.10%\nEpoch 27/50 | Train Loss: 0.0042 | Train Acc: 100.00% | Val Loss: 1.5322 | Val Acc: 65.90%\nEpoch 28/50 | Train Loss: 0.0062 | Train Acc: 100.00% | Val Loss: 1.5473 | Val Acc: 66.00%\nEpoch 29/50 | Train Loss: 0.0058 | Train Acc: 100.00% | Val Loss: 1.5391 | Val Acc: 66.70%\nEpoch 30/50 | Train Loss: 0.0059 | Train Acc: 100.00% | Val Loss: 1.5533 | Val Acc: 65.10%\nEpoch 31/50 | Train Loss: 0.0102 | Train Acc: 100.00% | Val Loss: 1.5580 | Val Acc: 65.70%\nEpoch 32/50 | Train Loss: 0.0112 | Train Acc: 100.00% | Val Loss: 1.5547 | Val Acc: 66.10%\nEpoch 33/50 | Train Loss: 0.0044 | Train Acc: 100.00% | Val Loss: 1.5592 | Val Acc: 65.80%\nEpoch 34/50 | Train Loss: 0.0066 | Train Acc: 100.00% | Val Loss: 1.5569 | Val Acc: 65.50%\nEpoch 35/50 | Train Loss: 0.0043 | Train Acc: 100.00% | Val Loss: 1.5710 | Val Acc: 64.70%\nEpoch 36/50 | Train Loss: 0.0047 | Train Acc: 100.00% | Val Loss: 1.5428 | Val Acc: 65.60%\nEpoch 37/50 | Train Loss: 0.0100 | Train Acc: 100.00% | Val Loss: 1.5808 | Val Acc: 65.70%\nEpoch 38/50 | Train Loss: 0.0103 | Train Acc: 100.00% | Val Loss: 1.5692 | Val Acc: 65.20%\nEpoch 39/50 | Train Loss: 0.0048 | Train Acc: 100.00% | Val Loss: 1.5700 | Val Acc: 65.20%\nEpoch 40/50 | Train Loss: 0.0104 | Train Acc: 100.00% | Val Loss: 1.5409 | Val Acc: 65.50%\nEpoch 41/50 | Train Loss: 0.0037 | Train Acc: 100.00% | Val Loss: 1.5462 | Val Acc: 65.20%\nEpoch 42/50 | Train Loss: 0.0049 | Train Acc: 100.00% | Val Loss: 1.5696 | Val Acc: 65.40%\nEpoch 43/50 | Train Loss: 0.0119 | Train Acc: 100.00% | Val Loss: 1.5765 | Val Acc: 65.20%\nEpoch 44/50 | Train Loss: 0.0078 | Train Acc: 100.00% | Val Loss: 1.5430 | Val Acc: 65.30%\nEpoch 45/50 | Train Loss: 0.0162 | Train Acc: 100.00% | Val Loss: 1.5482 | Val Acc: 65.40%\nEpoch 46/50 | Train Loss: 0.0092 | Train Acc: 100.00% | Val Loss: 1.5795 | Val Acc: 65.50%\nEpoch 47/50 | Train Loss: 0.0047 | Train Acc: 100.00% | Val Loss: 1.5494 | Val Acc: 65.60%\nEpoch 48/50 | Train Loss: 0.0093 | Train Acc: 100.00% | Val Loss: 1.5544 | Val Acc: 65.40%\nEpoch 49/50 | Train Loss: 0.0075 | Train Acc: 100.00% | Val Loss: 1.5662 | Val Acc: 64.80%\nEpoch 50/50 | Train Loss: 0.0041 | Train Acc: 100.00% | Val Loss: 1.5514 | Val Acc: 65.40%\nTraining completed!\nTraining completed in 1614.02 seconds\nEvaluating on test set...\nAccuracy: 0.6300\nPrecision: 0.6535\nRecall: 0.6300\nF1 Score: 0.6242\nTotal classes: 41\nFound 1000 valid audio files in /kaggle/input/filtereddata/audio/test\nError in experiment with 100 labeled samples per class: 'SeFOSS' object has no attribute 'eval'\n==================================================\nRunning experiment with 200 labeled samples per class\n==================================================\n==================================================\nRunning SeFOSS with 200 labeled samples per class\n==================================================\nTotal classes: 41\nFound 1000 valid audio files in /kaggle/input/filtereddata/audio/train\nFound 1000 valid audio files in /kaggle/input/filtereddata/audio/test\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/444710727.py\", line 1196, in main_fixed\n    analyze_model_performance(sefoss, test_loader, idx_to_class)\n  File \"/tmp/ipykernel_31/444710727.py\", line 934, in analyze_model_performance\n    model.eval()\n    ^^^^^^^^^^\nAttributeError: 'SeFOSS' object has no attribute 'eval'\n","output_type":"stream"},{"name":"stdout","text":"Found 1000 valid audio files in /kaggle/input/filtereddata/audio/val\nLabeled samples: 1000, Unlabeled samples: 0\nWarning: No unlabeled data available\nStarting SeFOSS training...\nPretraining model...\nPretrain Epoch 1/5 | Train Loss: 3.2625 | Train Acc: 15.50% | Val Loss: 2.7426 | Val Acc: 24.50%\nPretrain Epoch 2/5 | Train Loss: 2.1639 | Train Acc: 40.90% | Val Loss: 2.1217 | Val Acc: 40.70%\nPretrain Epoch 3/5 | Train Loss: 1.4518 | Train Acc: 60.30% | Val Loss: 1.9217 | Val Acc: 47.20%\nPretrain Epoch 4/5 | Train Loss: 1.0148 | Train Acc: 71.50% | Val Loss: 1.7180 | Val Acc: 53.20%\nPretrain Epoch 5/5 | Train Loss: 0.7385 | Train Acc: 79.90% | Val Loss: 1.7578 | Val Acc: 53.50%\nPretraining completed!\nComputing confidence thresholds...\nThresholds - tau_d: 0.9312, tau_ood: 0.6220, tau_nood: 0.3302\nEpoch 1/50 | Train Loss: 0.6204 | Train Acc: 93.60% | Val Loss: 1.6629 | Val Acc: 56.30%\nEpoch 2/50 | Train Loss: 0.4455 | Train Acc: 96.70% | Val Loss: 1.6728 | Val Acc: 56.50%\nEpoch 3/50 | Train Loss: 0.3242 | Train Acc: 98.30% | Val Loss: 1.6367 | Val Acc: 59.90%\nEpoch 4/50 | Train Loss: 0.2890 | Train Acc: 97.70% | Val Loss: 1.9163 | Val Acc: 55.70%\nEpoch 5/50 | Train Loss: 0.3013 | Train Acc: 94.30% | Val Loss: 1.8870 | Val Acc: 51.70%\nEpoch 6/50 | Train Loss: 0.2663 | Train Acc: 98.00% | Val Loss: 1.8751 | Val Acc: 56.90%\nEpoch 7/50 | Train Loss: 0.1642 | Train Acc: 99.40% | Val Loss: 1.5943 | Val Acc: 60.80%\nEpoch 8/50 | Train Loss: 0.1508 | Train Acc: 98.60% | Val Loss: 1.7248 | Val Acc: 60.90%\nEpoch 9/50 | Train Loss: 0.2406 | Train Acc: 98.40% | Val Loss: 1.9056 | Val Acc: 56.70%\nEpoch 10/50 | Train Loss: 0.1676 | Train Acc: 99.00% | Val Loss: 1.9138 | Val Acc: 58.20%\nEpoch 11/50 | Train Loss: 0.1151 | Train Acc: 99.20% | Val Loss: 1.8798 | Val Acc: 58.40%\nEpoch 12/50 | Train Loss: 0.1235 | Train Acc: 98.90% | Val Loss: 1.9268 | Val Acc: 59.10%\nEpoch 13/50 | Train Loss: 0.1589 | Train Acc: 98.60% | Val Loss: 2.0356 | Val Acc: 56.70%\nEpoch 14/50 | Train Loss: 0.0747 | Train Acc: 99.90% | Val Loss: 1.6679 | Val Acc: 60.90%\nEpoch 15/50 | Train Loss: 0.0285 | Train Acc: 100.00% | Val Loss: 1.5779 | Val Acc: 62.90%\nEpoch 16/50 | Train Loss: 0.0130 | Train Acc: 100.00% | Val Loss: 1.5657 | Val Acc: 62.20%\nEpoch 17/50 | Train Loss: 0.0107 | Train Acc: 100.00% | Val Loss: 1.5555 | Val Acc: 63.80%\nEpoch 18/50 | Train Loss: 0.0237 | Train Acc: 100.00% | Val Loss: 1.5991 | Val Acc: 63.70%\nEpoch 19/50 | Train Loss: 0.0326 | Train Acc: 99.70% | Val Loss: 1.6679 | Val Acc: 62.40%\nEpoch 20/50 | Train Loss: 0.0120 | Train Acc: 99.80% | Val Loss: 1.5905 | Val Acc: 64.50%\nEpoch 21/50 | Train Loss: 0.0118 | Train Acc: 100.00% | Val Loss: 1.5936 | Val Acc: 64.50%\nEpoch 22/50 | Train Loss: 0.0108 | Train Acc: 100.00% | Val Loss: 1.6290 | Val Acc: 64.30%\nEpoch 23/50 | Train Loss: 0.0044 | Train Acc: 100.00% | Val Loss: 1.6013 | Val Acc: 64.60%\nEpoch 24/50 | Train Loss: 0.0035 | Train Acc: 100.00% | Val Loss: 1.5778 | Val Acc: 65.70%\nEpoch 25/50 | Train Loss: 0.0089 | Train Acc: 100.00% | Val Loss: 1.5847 | Val Acc: 65.50%\nEpoch 26/50 | Train Loss: 0.0036 | Train Acc: 100.00% | Val Loss: 1.5810 | Val Acc: 65.50%\nEpoch 27/50 | Train Loss: 0.0037 | Train Acc: 100.00% | Val Loss: 1.5748 | Val Acc: 65.30%\nEpoch 28/50 | Train Loss: 0.0048 | Train Acc: 100.00% | Val Loss: 1.5929 | Val Acc: 64.80%\nEpoch 29/50 | Train Loss: 0.0040 | Train Acc: 100.00% | Val Loss: 1.5914 | Val Acc: 66.00%\nEpoch 30/50 | Train Loss: 0.0462 | Train Acc: 100.00% | Val Loss: 1.5925 | Val Acc: 65.90%\nEpoch 31/50 | Train Loss: 0.0100 | Train Acc: 100.00% | Val Loss: 1.5782 | Val Acc: 65.40%\nEpoch 32/50 | Train Loss: 0.0070 | Train Acc: 100.00% | Val Loss: 1.5976 | Val Acc: 64.60%\nEpoch 33/50 | Train Loss: 0.0062 | Train Acc: 100.00% | Val Loss: 1.6150 | Val Acc: 64.40%\nEpoch 34/50 | Train Loss: 0.0092 | Train Acc: 100.00% | Val Loss: 1.6275 | Val Acc: 64.80%\nEpoch 35/50 | Train Loss: 0.0056 | Train Acc: 100.00% | Val Loss: 1.6248 | Val Acc: 64.40%\nEpoch 36/50 | Train Loss: 0.0032 | Train Acc: 100.00% | Val Loss: 1.6190 | Val Acc: 64.70%\nEpoch 37/50 | Train Loss: 0.0178 | Train Acc: 100.00% | Val Loss: 1.6496 | Val Acc: 64.40%\nEpoch 38/50 | Train Loss: 0.0164 | Train Acc: 100.00% | Val Loss: 1.6259 | Val Acc: 64.50%\nEpoch 39/50 | Train Loss: 0.0074 | Train Acc: 100.00% | Val Loss: 1.6519 | Val Acc: 64.40%\nEpoch 40/50 | Train Loss: 0.0029 | Train Acc: 100.00% | Val Loss: 1.6219 | Val Acc: 64.20%\nEpoch 41/50 | Train Loss: 0.0040 | Train Acc: 100.00% | Val Loss: 1.6349 | Val Acc: 64.70%\nEpoch 42/50 | Train Loss: 0.0072 | Train Acc: 100.00% | Val Loss: 1.6205 | Val Acc: 64.30%\nEpoch 43/50 | Train Loss: 0.0073 | Train Acc: 100.00% | Val Loss: 1.6183 | Val Acc: 64.90%\nEpoch 44/50 | Train Loss: 0.0136 | Train Acc: 100.00% | Val Loss: 1.6489 | Val Acc: 64.40%\nEpoch 45/50 | Train Loss: 0.0135 | Train Acc: 100.00% | Val Loss: 1.6295 | Val Acc: 65.30%\nEpoch 46/50 | Train Loss: 0.0046 | Train Acc: 100.00% | Val Loss: 1.6346 | Val Acc: 65.00%\nEpoch 47/50 | Train Loss: 0.0110 | Train Acc: 100.00% | Val Loss: 1.6126 | Val Acc: 65.00%\nEpoch 48/50 | Train Loss: 0.0035 | Train Acc: 100.00% | Val Loss: 1.6150 | Val Acc: 65.50%\nEpoch 49/50 | Train Loss: 0.0047 | Train Acc: 100.00% | Val Loss: 1.6102 | Val Acc: 65.40%\nEpoch 50/50 | Train Loss: 0.0038 | Train Acc: 100.00% | Val Loss: 1.6072 | Val Acc: 65.20%\nTraining completed!\nTraining completed in 1612.81 seconds\nEvaluating on test set...\nAccuracy: 0.6870\nPrecision: 0.6981\nRecall: 0.6870\nF1 Score: 0.6808\nTotal classes: 41\nFound 1000 valid audio files in /kaggle/input/filtereddata/audio/test\nError in experiment with 200 labeled samples per class: 'SeFOSS' object has no attribute 'eval'\n==================================================\nRunning experiment with 400 labeled samples per class\n==================================================\n==================================================\nRunning SeFOSS with 400 labeled samples per class\n==================================================\nTotal classes: 41\nFound 1000 valid audio files in /kaggle/input/filtereddata/audio/train\nFound 1000 valid audio files in /kaggle/input/filtereddata/audio/test\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/444710727.py\", line 1196, in main_fixed\n    analyze_model_performance(sefoss, test_loader, idx_to_class)\n  File \"/tmp/ipykernel_31/444710727.py\", line 934, in analyze_model_performance\n    model.eval()\n    ^^^^^^^^^^\nAttributeError: 'SeFOSS' object has no attribute 'eval'\n","output_type":"stream"},{"name":"stdout","text":"Found 1000 valid audio files in /kaggle/input/filtereddata/audio/val\nLabeled samples: 1000, Unlabeled samples: 0\nWarning: No unlabeled data available\nStarting SeFOSS training...\nPretraining model...\nPretrain Epoch 1/5 | Train Loss: 3.2326 | Train Acc: 14.90% | Val Loss: 2.7691 | Val Acc: 27.20%\nPretrain Epoch 2/5 | Train Loss: 2.2021 | Train Acc: 40.60% | Val Loss: 2.1556 | Val Acc: 37.20%\nPretrain Epoch 3/5 | Train Loss: 1.4844 | Train Acc: 59.20% | Val Loss: 1.9435 | Val Acc: 49.60%\nPretrain Epoch 4/5 | Train Loss: 1.0409 | Train Acc: 69.70% | Val Loss: 1.8418 | Val Acc: 50.10%\nPretrain Epoch 5/5 | Train Loss: 0.6973 | Train Acc: 81.10% | Val Loss: 1.7808 | Val Acc: 54.90%\nPretraining completed!\nComputing confidence thresholds...\nThresholds - tau_d: 0.9765, tau_ood: 0.7561, tau_nood: 0.4499\nEpoch 1/50 | Train Loss: 0.4730 | Train Acc: 94.60% | Val Loss: 1.7766 | Val Acc: 56.10%\nEpoch 2/50 | Train Loss: 0.3361 | Train Acc: 95.20% | Val Loss: 1.9029 | Val Acc: 56.00%\nEpoch 3/50 | Train Loss: 0.2586 | Train Acc: 96.90% | Val Loss: 1.7677 | Val Acc: 57.00%\nEpoch 4/50 | Train Loss: 0.1958 | Train Acc: 98.60% | Val Loss: 1.7087 | Val Acc: 58.10%\nEpoch 5/50 | Train Loss: 0.2036 | Train Acc: 97.20% | Val Loss: 1.8256 | Val Acc: 56.50%\nEpoch 6/50 | Train Loss: 0.2270 | Train Acc: 95.50% | Val Loss: 1.9905 | Val Acc: 56.00%\nEpoch 7/50 | Train Loss: 0.2190 | Train Acc: 97.00% | Val Loss: 1.9408 | Val Acc: 55.70%\nEpoch 8/50 | Train Loss: 0.1910 | Train Acc: 97.90% | Val Loss: 1.8937 | Val Acc: 56.40%\nEpoch 9/50 | Train Loss: 0.2180 | Train Acc: 94.80% | Val Loss: 1.9611 | Val Acc: 56.40%\nEpoch 10/50 | Train Loss: 0.1670 | Train Acc: 97.60% | Val Loss: 2.0299 | Val Acc: 55.90%\nEpoch 11/50 | Train Loss: 0.1108 | Train Acc: 99.80% | Val Loss: 1.7382 | Val Acc: 60.20%\nEpoch 12/50 | Train Loss: 0.0400 | Train Acc: 100.00% | Val Loss: 1.6556 | Val Acc: 62.40%\nEpoch 13/50 | Train Loss: 0.0339 | Train Acc: 99.80% | Val Loss: 1.7041 | Val Acc: 63.20%\nEpoch 14/50 | Train Loss: 0.0189 | Train Acc: 100.00% | Val Loss: 1.6334 | Val Acc: 64.20%\nEpoch 15/50 | Train Loss: 0.0242 | Train Acc: 99.90% | Val Loss: 1.5845 | Val Acc: 64.40%\nEpoch 16/50 | Train Loss: 0.0251 | Train Acc: 100.00% | Val Loss: 1.6472 | Val Acc: 64.20%\nEpoch 17/50 | Train Loss: 0.0197 | Train Acc: 100.00% | Val Loss: 1.6286 | Val Acc: 65.20%\nEpoch 18/50 | Train Loss: 0.0093 | Train Acc: 100.00% | Val Loss: 1.5720 | Val Acc: 65.50%\nEpoch 19/50 | Train Loss: 0.0188 | Train Acc: 100.00% | Val Loss: 1.6156 | Val Acc: 64.50%\nEpoch 20/50 | Train Loss: 0.0147 | Train Acc: 100.00% | Val Loss: 1.6658 | Val Acc: 64.30%\nEpoch 21/50 | Train Loss: 0.0108 | Train Acc: 100.00% | Val Loss: 1.6507 | Val Acc: 64.50%\nEpoch 22/50 | Train Loss: 0.0051 | Train Acc: 100.00% | Val Loss: 1.6502 | Val Acc: 64.70%\nEpoch 23/50 | Train Loss: 0.0059 | Train Acc: 100.00% | Val Loss: 1.6641 | Val Acc: 65.20%\nEpoch 24/50 | Train Loss: 0.0047 | Train Acc: 100.00% | Val Loss: 1.6369 | Val Acc: 66.30%\nEpoch 25/50 | Train Loss: 0.0093 | Train Acc: 100.00% | Val Loss: 1.6389 | Val Acc: 65.00%\nEpoch 26/50 | Train Loss: 0.0184 | Train Acc: 100.00% | Val Loss: 1.6442 | Val Acc: 64.70%\nEpoch 27/50 | Train Loss: 0.0076 | Train Acc: 100.00% | Val Loss: 1.6277 | Val Acc: 64.30%\nEpoch 28/50 | Train Loss: 0.0179 | Train Acc: 100.00% | Val Loss: 1.6275 | Val Acc: 64.80%\nEpoch 29/50 | Train Loss: 0.0551 | Train Acc: 100.00% | Val Loss: 1.6423 | Val Acc: 64.40%\nEpoch 30/50 | Train Loss: 0.0130 | Train Acc: 100.00% | Val Loss: 1.6838 | Val Acc: 63.40%\nEpoch 31/50 | Train Loss: 0.0082 | Train Acc: 100.00% | Val Loss: 1.6448 | Val Acc: 63.50%\nEpoch 32/50 | Train Loss: 0.0168 | Train Acc: 100.00% | Val Loss: 1.6300 | Val Acc: 63.80%\nEpoch 33/50 | Train Loss: 0.0529 | Train Acc: 100.00% | Val Loss: 1.6455 | Val Acc: 63.90%\nEpoch 34/50 | Train Loss: 0.0069 | Train Acc: 100.00% | Val Loss: 1.6317 | Val Acc: 64.40%\nEpoch 35/50 | Train Loss: 0.0079 | Train Acc: 100.00% | Val Loss: 1.6389 | Val Acc: 65.00%\nEpoch 36/50 | Train Loss: 0.0063 | Train Acc: 100.00% | Val Loss: 1.6302 | Val Acc: 64.90%\nEpoch 37/50 | Train Loss: 0.0089 | Train Acc: 100.00% | Val Loss: 1.6533 | Val Acc: 65.20%\nEpoch 38/50 | Train Loss: 0.0043 | Train Acc: 100.00% | Val Loss: 1.6043 | Val Acc: 65.60%\nEpoch 39/50 | Train Loss: 0.0039 | Train Acc: 100.00% | Val Loss: 1.6015 | Val Acc: 65.10%\nEpoch 40/50 | Train Loss: 0.0055 | Train Acc: 100.00% | Val Loss: 1.6151 | Val Acc: 65.40%\nEpoch 41/50 | Train Loss: 0.0048 | Train Acc: 100.00% | Val Loss: 1.6156 | Val Acc: 66.20%\nEpoch 42/50 | Train Loss: 0.0037 | Train Acc: 100.00% | Val Loss: 1.6177 | Val Acc: 66.20%\nEpoch 43/50 | Train Loss: 0.0032 | Train Acc: 100.00% | Val Loss: 1.6040 | Val Acc: 65.60%\nEpoch 44/50 | Train Loss: 0.0025 | Train Acc: 100.00% | Val Loss: 1.6318 | Val Acc: 65.80%\nEpoch 45/50 | Train Loss: 0.0030 | Train Acc: 100.00% | Val Loss: 1.5931 | Val Acc: 66.20%\nEpoch 46/50 | Train Loss: 0.0020 | Train Acc: 100.00% | Val Loss: 1.6023 | Val Acc: 65.70%\nEpoch 47/50 | Train Loss: 0.0117 | Train Acc: 100.00% | Val Loss: 1.6063 | Val Acc: 66.10%\nEpoch 48/50 | Train Loss: 0.0148 | Train Acc: 100.00% | Val Loss: 1.6149 | Val Acc: 66.40%\nEpoch 49/50 | Train Loss: 0.0033 | Train Acc: 100.00% | Val Loss: 1.6008 | Val Acc: 66.10%\nEpoch 50/50 | Train Loss: 0.0032 | Train Acc: 100.00% | Val Loss: 1.6138 | Val Acc: 65.40%\nTraining completed!\nTraining completed in 1619.82 seconds\nEvaluating on test set...\nAccuracy: 0.6500\nPrecision: 0.6617\nRecall: 0.6500\nF1 Score: 0.6391\nTotal classes: 41\nFound 1000 valid audio files in /kaggle/input/filtereddata/audio/test\nError in experiment with 400 labeled samples per class: 'SeFOSS' object has no attribute 'eval'\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/444710727.py\", line 1196, in main_fixed\n    analyze_model_performance(sefoss, test_loader, idx_to_class)\n  File \"/tmp/ipykernel_31/444710727.py\", line 934, in analyze_model_performance\n    model.eval()\n    ^^^^^^^^^^\nAttributeError: 'SeFOSS' object has no attribute 'eval'\n","output_type":"stream"},{"name":"stdout","text":"Experiment completed successfully!\n   Labeled Samples Per Class  Accuracy  Precision  Recall  F1 Score\n0                         40       0.0        0.0     0.0       0.0\n1                        100       0.0        0.0     0.0       0.0\n2                        200       0.0        0.0     0.0       0.0\n3                        400       0.0        0.0     0.0       0.0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport zipfile\n\n# Define the directory to zip and the output zip file path\ndir_to_zip = \"/kaggle/working/SEFOSS_AudioData\"\nzip_path = \"/kaggle/working/SEFOSS_AudioData.zip\"\n\n# Create a zip file\nwith zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for root, dirs, files in os.walk(dir_to_zip):\n        for file in files:\n            file_path = os.path.join(root, file)\n            # Add file to zip, maintaining relative path\n            zipf.write(file_path, os.path.relpath(file_path, dir_to_zip))\n\nprint(f\"Zipped output directory to: {zip_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}